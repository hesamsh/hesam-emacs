#+TITLE: Learning Models in Optimization Algorithms
#+AUTHOR: Hesam Shams,  Oleg Shylo
#+EMAIL: 
#+OPTIONS: date:nil
#+OPTIONS:   H:5 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+LATEX_CLASS: article
#+EXCLUDE_TAGS: NOEXPORT

#+begin_abstract
This paper explores the applications of learning models to design algorithms for binary optimization problems. The logistic regression learning model is used to construct a directional tabu algorithm (DTA). We test the algorithm on benchmark instances of the job shop scheduling problem. Using this experiments, we demonstrate that the inclusion of the logistic regression model into the tabu search method provides significant boost its performance. 
#+end_abstract

* Latex Headers                                                    :NOEXPORT:
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage[onehalfspacing]{setspace}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[tight,hang,nooneline,raggedright,figtopcap]{subfigure}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{multicol}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{amsmath,amssymb}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage{multimedia}
#+LATEX_HEADER: \usepackage[boxed]{algorithm}
#+LATEX_HEADER: \usepackage{algorithmic}
#+LATEX_HEADER: \usepackage{natbib}

* Emacs Headers                                                    :NOEXPORT:
#+BEGIN_SRC emacs-lisp
  (require 'ox-latex)
  (setq org-latex-listings t)
  (setq org-latex-prefer-user-labels t)
  (add-to-list 'org-latex-packages-alist '("" "listings"))
  (add-to-list 'org-latex-packages-alist '("" "color"))

  (defun remove-orgmode-latex-labels1(text backend info)
     "Remove labels generated by org-mode"
     (replace-regexp-in-string 
        "\\\\label{sec:org\.*}" "" text)    
   )

   (defun remove-orgmode-latex-labels2(text backend info)
     "Remove labels generated by org-mode"
     (replace-regexp-in-string 
        "\\\\\(" "$" text)     
   )

   (defun remove-orgmode-latex-labels3(text backend info)
     (replace-regexp-in-string 
       "\\\\\)" "$" text)   
   )

  (add-hook 'org-export-latex-final-hook 'remove-orgmode-latex-labels)
   (add-to-list 'org-export-filter-final-output-functions
                    'remove-orgmode-latex-labels1)
   (add-to-list 'org-export-filter-final-output-functions
                    'remove-orgmode-latex-labels2)
   (add-to-list 'org-export-filter-final-output-functions
                    'remove-orgmode-latex-labels3)
#+END_SRC

#+RESULTS:
| remove-orgmode-latex-labels3 | remove-orgmode-latex-labels2 | remove-orgmode-latex-labels1 |

#+BEGIN_SRC emacs-lisp
(setq org-latex-pdf-process  '("xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "bibtex %b"
        "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
#+END_SRC

#+RESULTS:
| xelatex -shell-escape -interaction nonstopmode -output-directory %o %f | bibtex %b | xelatex -shell-escape -interaction nonstopmode -output-directory %o %f | xelatex -shell-escape -interaction nonstopmode -output-directory %o %f |

- set the size of the formulas when displayed inline in org
#+BEGIN_SRC emacs-lisp
(setq org-format-latex-options (plist-put org-format-latex-options :scale 3.0))
#+end_src

#+RESULTS:
| :foreground | default | :background | default | :scale | 3.0 | :html-foreground | Black | :html-background | Transparent | :html-scale | 1.0 | :matchers | (begin $1 $ $$ \( \[) |

#+BEGIN_SRC emacs-lisp
(add-hook 'python-mode-hook 'guess-style-guess-tabs-mode)
   (add-hook 'python-mode-hook (lambda ()
                                    (guess-style-guess-tab-width)))
#+END_SRC

#+RESULTS:
| (lambda nil (guess-style-guess-tab-width)) | guess-style-guess-tabs-mode | (lambda nil (set (make-local-variable (quote yas-indent-line)) (quote fixed))) | wisent-python-default-setup |

- display inline images in org

#+BEGIN_SRC emacs-lisp
(setq org-startup-with-inline-images t)
(setq org-image-actual-width nil)
(org-redisplay-inline-images)
#+END_SRC

#+RESULTS:
: No images to display inline

* Remove all Results                                               :NOEXPORT:
#+begin_src emacs-lisp
(org-babel-map-src-blocks nil (org-babel-remove-result))
#+end_src

* Introduction

When considering successful applications, the tabu search method
\citep{Glover:1989} is arguably one of the best standalone
optimization approaches among those based on the local search, where a
set of moves transform one solution into another through modification
of their constituent attributes. Tabu search employs a short-term
memory prohibition mechanism, a rule that prevents revisiting of
solution attributes recently removed from the current solution. Less
commonly, tabu restrictions inhibit removal of attributes that were
recently introduced into the current solution. In general, these two
types of restrictions lead to different search trajectories and might
be employed in parallel, however in case of 0-1 optimization problems
they are equivalent \citep{Glover:1989}. Through inhibition mechanisms
and by enabling non-improving solution attributes, the tabu search
method provides an almost effortless escape from local minima together
with efficient exploration of alternative solutions.

Typically, when a certain attribute enters a list of prohibited
attributes, the tabu list, it will remain there for a fixed number of
iterations determined by a /tabu tenure/ parameter. Most tabu search
implementations adopt a single tabu tenure parameter for each of the
solution attributes, which is often defined as a function of problem
size and might be dynamically adjusted to avoid cycling effects
\citep{Battiti:1994}. The attribute-dependent tenures, where each
solution attribute is assigned a separate tabu tenure value, has been
also identified in earlier publications
\citep{Glover:1993,Glover:1990a}. However, previous discussions of the
attribute-dependent tenures mainly focused on the variability with
respect to restrictive powers of different move attributes
\citep{Glover:1993}, with an emphasis being placed on an idea that
when using the same tabu tenure for all solution components,
prohibition of certain solution attributes might have a stronger
impact on search process than prohibition of the others. 

Many optimization approaches rely on the tabu method, but often
utilize additional mechanisms for diversification and intensification
of the search. For example, multi-start tabu strategies repeatedly
launch the tabu search procedure using different initial solutions. In
the path-relinking framework one collects a set of diverse
high-quality solutions, the elite set, constructs paths between them,
and explores the neighborhoods of the intermediate solutions using
local search or tabu search procedures. However, when implementing a
path-relinking algorithm, there are many questions that are not easy
to answer: what is the optimal size of the elite set, how much time
should be spend constructing the elite set versus exploring the paths
between them.

In this paper, we propose to integrate the path-relinking stage with
the main tabu search procedure by embedding the long term memory into
the tabu list mechanism. Instead of using a single tabu tenure
parameter, each component of a solution vector is assigned a separate
tabu tenure value that is dynamically updated and depends on
previously found solutions. To define the values for tabu tenures, we
propose to use an approximation to the Boltzmann's distribution. The
proposed algorithm is inspired by the Global Equilibrium Search
method, and can be considered as a crossover between tabu search and
global equilibrium search.

* Machine Learning in Binary Optimization 

Local search methods iteratively move from one solution to another using the values of the corresponding objective values for guidance. In the process, these algorithms evaluate numerous solutions, but ignore most of the infromation they present. Instead, this infromation can be analyzed for consistent patterns to guide the search process. In this section we describe the statistical prediction model for binary optimization problems, and describe a simple, scalable implementation.

** Statistical Model

A binary optimization problem can be formulated as 


\begin{equation}\label{general.model}
\begin{array}{cc}
\text{minimize } f(x) \\
\text{s.t. } x \in S \subset \{0,1\}^n
\end{array}
\end{equation}


Since each component of a feasible vector $x$ is either 0 or 1, the index set $\overline{1,n} \equiv \{1,\ldots,n\}$ can be split into two classes based on an optimal solution $x^*$ to the problem (\ref{general.model}). To the first class we assign all indexes for which the component value is equal to one, while the remaining indexes are assigned to the second class. Formally, these classes are denoted $\mathcal C^1=\{j | x_j^*=1\}$ and $\mathcal C^0=\{j | x_j^*=0\}$, $j\in \overline{1,n}$. Clearly, there are as many such partitions as there are optimal solutions.

As one solves the problem (\ref{general.model}), some subset of feasible solutions gets discovered. Let $I_j(t)$ denote a vector of discovered information about the variable $x_j$ that is formed by jth components of the set of feasible solutions, $x^1, \ldots, x^m\in S$, representing solutions visited by an algorithm $\mathcal A$ after time $t$, and their corresponding objectives, $f(x_1), \ldots, f(x_m)$:

\begin{equation}\label{infromation.vec}
I_j(t)=[ (x^1_j, f(x^1)), (x^2_j,f(x^2)), \ldots, (x^m_j, f(x^m))]
\end{equation}


Assuming that each time the algorithm produces a different information vector $I_j(t)$ after time $t$, we would like to classify each vector $I_j(t)$ either as belonging to $\mathcal C_1$ or $\mathcal C_0$. In other words, we would like to build a prediction model $M$ that would map vectors $I_j(t)$, $j \in \overline{1,n}$, into the interval [0,1], returning a conditional probability $Pr(j\in \mathcal C^1 | I_j(t))$. If there are no consistent patterns in $I_j(t)$, the model should return the probabilities close to 0.5, otherwise the probabilities may get closer to the ends of the interval [0,1], yileding predictions that can guide the search. 

The applications have to implement the prediction models using some reductions of $I_j(t)$ due to memory constraints. Next, we present a a simple, scalable implementation based on the logistic regression method.

** Logistic Regression Model

*** Average Difference 

-- idea: we have average solution quality with xj=0 and xj=1, next we build regression model on top of this



*** Difference of 

Given $I_j(t)$ (defined in (\ref{infromation.vec}), we can calulcate the minimum objective value among visited solutions with $x_j=1$, and the minimum objective among solutions with $x_j=0$:
$$
D^1_j(t)=\min \{ f(x_j^k) | (x_j^k, f(x^k)) \in I_j(t), x_j^k=1\}
$$
and 
$$
D^0_j(t)=\min \{ f(x_j^k) | (x_j^k, f(x^k)) \in I_j(t), x_j^k=0\}
$$
We define the reduced information vector as $I^R_j(t)=[D^1_j(t), D^0_j(t)]\in \mathcal R^2$. Hence, we reduce $I_j(t)$ just to a vector of size two. Clearly, instead of storing $I_j(t)$ in the memory for these calculations, $I_j^R(t)$ should be updated directly every time the algorithm finds a new feasible solution.

* Description of the approach

problem of data accumulationour approach for data collection and pattern analysis.

a statisitcal model detect patterns in 

** Main Idea


In a simplest form, the tabu search algorithm iteratively moves from one solution to another using the values of the corresponding objective values for guidance. Given a current solution $x$, at each iteration the algorithm moves to one of the solutions in its neighborhood $N(x)$, however the tabu search method prohibits some of the solutions in $N(x)$. Suppose that $latestChange(j)$ is the latest iteration when the solution component $j$ changed its value, then any solution in $N(x)$ that differs from $x$ in $jth$ component is prohibited until the iteration number $latestChange(j)+tenure$, where $tenure$ defines a length of the tabu period. There are many variations of tabu search implementations, but the idea is similar: prohibit changes in components that were recently modified. Typically, the best non-tabu solution in $N(x)$ is chosen as a next current solution, and after that the process repeats.

In the current paper, we explore an approach that uses the tabu prohibition mechanism both for escaping from local minima, and for guiding the search to promising solution areas. Instead of a single tabu parameter, each solution component is assigned its own tabu parameter $tabu_j$ that is dynamically updated during the search. By assigning large values to $tabu_j$ the algorithm attempts to preserve the current value of the $x_j$, while small $tabu_j$ will indicate that the component $x_j$ can be modified at a faster pace. For example, if we wish to guide the tabu search to a specified solution $x^*$, we can use a standard tabu search procedure, but whenever $x_j$ takes the same value as $x^*_j$, we would set $tabu_j$ to $T^{U}$, and set it to $T^{L}$ if the new $x_j$ is different from $x^*_j$, where $T^{U}>T^{L}$. If the neighborhood is connected (any solution can be reached from any other solution), then an appropriate choice of $T^{L}$ and $T^{U}$ will guarantee the convergence.


** Long-term Memory 
\label{long.term.memory}

To accumulate information about the search space, we will maintain an
approximation to the Boltzmann's distribution defined for the
optimization problem given in (\ref{general.model}). Similar
approximation was first introduced within the Global Equilibrium
Search method. In this distribution (\ref{eq:GE}) the random vector
$\xi$ takes values from the set of all feasible solution $S$ and the
probability mass function depend on a temperature parameter $\mu$:
\begin{equation}
P\{\xi=x\}=\frac{ e^{-\mu f(x)}}{\displaystyle \sum_{x\in S} e^{-\mu  f(x)}} \label{eq:GE}
\end{equation}
or, if considering each component separately we have \ref{distribution.2}:
\begin{equation}
P\{\xi_j=1\}=\frac{\displaystyle  \sum_{x\in S^1_j} e^{-\mu f(x)}}{\displaystyle  \sum_{x\in S} e^{-\mu f(x)}} \label{distribution.2}
\end{equation}
where $S$ is the set of feasible solutions and $S^1_j$ is the set of
solutions with $j$-th component equal to 1.  The larger temperature
values lead to the distributions that have higher probabilities
assigned to better solutions, while zero temperature produces a
uniform distribution on the set of all feasible solutions.

To approximate (\ref{distribution.2}), we will use up to $l$ entries
for the sums in the denominator and enumerator using only the latest
solutions and the best solution found by the search procedure.  For
each solution component $j$, let $f^1_j$ ($f^0_j$) be the best found
objective for the solution with $x_j=1$  $(x_j=0)$. Consider two sets
of objective values, $H^1_j$ and $H^0_j$, which contain the most
recent objective values corresponding to the solutions having the
$j$-th component equal to 1 and 0, respectively. The formulation for
$H^1_j$ and $H^0_j$ are show in \ref{listH0} and \ref{listH1}
accordingly.
\begin{align}
H^1_j &= \{h^{1,j}_0,h^{1,j}_1,\ldots, h^{1,j}_{l_1}\}, (l_1 \leq l) \label{listH0} \\
H^0_j &= \{h^{0,j}_0,h^{0,j}_1,\ldots, h^{0,j}_{l_0}\}, (l_0 \leq l) \label{listH1}
\end{align}

Since the algorithm might never find a feasible solution with $j$-th
component equal to 0 (or 1), either $l_0$ or $l_1$ might be less
than $l$. Every time a new solution is encountered, these sets can be
updated in a constant time using linked lists. Overall, this will
require storing approximately $2n\cdot l$ objective values plus some
overhead for implementation of linked lists.

Now, we can approximate Boltzmann's probabilities using the equations
in \ref{eq:numerator}, \ref{eq:denominator}, and
\ref{approximation.probability}.
\begin{align}
Z_j^1(\mu) =& \exp\left(\mu\left[f(x^{min})-f^1_j\right]\right)+\sum\limits_{k=1}^{\min\left\{|H_j^1|,|H_j^0|\right\}} \exp\left(\mu\left[f(x^{min})-h_k^{1,j}\right]\right) \label{eq:numerator} \\
Z_j(\mu) =& \exp\left(\mu\left[f(x^{min})-f^1_j\right]\right)+\exp\left(\mu\left[f(x^{min})-f^0_j\right]\right) \nonumber \\
&+ \sum\limits_{k=1}^{\min\left\{|H_j^1|, |H_j^0|\right\}}\exp\left(\mu\left[f(x^{min})-h_k^{1,j}\right]\right) \nonumber \\
&+ \sum\limits_{k=1}^{\min\left\{|H_j^1|, |H_j^0|\right\}}\exp\left(\mu\left[f(x^{min})-h_k^{0,j}\right]\right) \label{eq:denominator} \\
\tilde{p}_j(\mu) =& \frac{Z^1_j(\mu)}{Z_j(\mu)} \label{approximation.probability}
\end{align}

The sum in (\ref{eq:numerator}) is a partial sum corresponding to the
numerator in (\ref{distribution.2}), while (\ref{eq:denominator}) is
the partial sum corresponding to the denominator of the expression in
(\ref{distribution.2}). When using this approximation, we need to
store $n$ values for $x^{min}$, $2n$ values for the vectors $f^0$ and
$f^1$, and up to $2nl$ values for storing $H^1$ and $H^0$. As the
temperature parameter $\mu$ increases, the probability vector
$\tilde{p}(\mu)$ is converging to the solution vector $x_{min}$,
however the convergence rate is different for every solution
component: the components that are common for all high-quality
solutions will converge faster than all the others. Thus, when
increasing the temperature parameter $\mu$, the probability value
change from 0.5 (no bias towards 0 or 1) to the value that is typical
to the best encountered solutions. The main advantage of this long
term memory implementation is that there is no need to store the
solutions explicitly.

** Dynamic tabu search tenure

In the proposed approach, an approximation to the Boltzmann's
distribution from Section \ref{long.term.memory} defines dynamic tabu
tenures. Whenever $x_j$ is modified, we compare its new value to the
current best solution $x^{best}_j$. If the probability
in(\ref{approximation.probability}) is close to 1 or 0 and the new
value is the same as $x^{best}_j$, then we assign a large tenure value
to $x_j$.  Otherwise, we want to enforce a faster rate of change
for $x_j$, so we assign a smaller tenure value. Next, we define a
function that link probabilities to tabu tenures.

*** Quadratic Tenure Function
After every local search transition, the tenure for each component
that has changed is determined by the quadratic function in \ref{tenure.formula1}.

\begin{equation}
 \mathrm{tabu}_j(\tilde{p}_j(\mu)) = \left\{
\begin{array}{ll}
      4(T^{U}-T^{L}) \tilde{p}_j(\mu)^2-4(T^{U}-T^{L})\tilde{p}_j(\mu)+T^{U} & x_j = x^{best}_j \\
      T^{L}  & x_j \neq x^{best}_j \\
\end{array} 
\right. \label{tenure.formula1}
\end{equation} 
where an interval $[T^{L},T^{U}]$ defines a range of possible tenure
values. The coefficients of this quadratic function are chosen to
satisfy the conditions in \ref{cond1}, \ref{cond2}, and \ref{cond3}.
\begin{align}
\mathrm{tabu}_j(\tilde{p}_j(\mu) ) &= T^{U} \text{ if } \tilde{p}_j(\mu)=1 \label{cond1} \\
\mathrm{tabu}_j(\tilde{p}_j(\mu) ) &= T^{U} \text{ if } \tilde{p}_j(\mu)=0 \label{cond2} \\
\mathrm{tabu}_j(\tilde{p}_j(\mu)) &= T^{L} \text{ if } \tilde{p}_j(\mu)=0.5 \label{cond3}
\end{align}

If the $j$-th component is set to a different value than the $j$-th
component in the best known solution, then the variable $j$ is
assigned a low tenure value $T^{L}$. Otherwise, the tenure value is a
quadratic function of the probabilities that approximate Boltzmann's
distribution: the closer probability $\tilde{p}_j$ is to 1 or 0, the
larger is the value of the assigned tabu tenure, with the maximum
possible value of $T^{U}$.

*** Other possible choices for the tenure function
**** Sigmoid Tenure Function

Similarly to the quadratic function, the assigned tenures belong to
the interval $[T^{L},T^{U}]$. Whenever a solution component $x_j$ is
modified its tenure is determined by the function in
\ref{tenure.formula2}.
\begin{equation}
 \mathrm{tabu}_j(\tilde{p}_j(\mu)) = \left\{
\begin{array}{ll}
      \frac{2T^{U}+T^L\exp(\alpha \tilde{p}_j(\mu))-T^L}{1+\exp(\alpha \tilde{p}_j(\mu))}& x_j = x^{best}_j,\tilde{p}_j(\mu)\leq 0.5\\
      \frac{2T^{U}+T^L\exp(\alpha (1-\tilde{p}_j(\mu)))-T^L}{1+\exp(\alpha(1-\tilde{p}_j(\mu)))}& x_j = x^{best}_j,\tilde{p}_j(\mu)>0.5\\
      T^{L}  & x_j \neq x^{best}_j \\
\end{array} 
\right. \label{tenure.formula2}
\end{equation}

This function is equal to $T^{U}$ for when $\tilde{p}_j(\mu)$  equals
to 1 and 0 as is shown in \ref{func:sigm}.
\begin{align}
\mathrm{tabu}_j(\tilde{p}_j(\mu)) &= T^{U} \text{ if } \tilde{p}_j(\mu)=1 \nonumber \\
\mathrm{tabu}_j(\tilde{p}_j(\mu)) &= T^{U} \text{ if } \tilde{p}_j(\mu)=0 \label{func:sigm}
\end{align}
where parameter $\alpha>0$ defines the steepness of the function, and
it should be chosen to satisfy the condition in \ref{cond:sigm}.
\begin{align}
\mathrm{tabu}_j(\tilde{p}_j(\mu)) &\approx T^{L} \qquad \text{ if } \quad \tilde{p}_j(\mu)=0.5 \label{cond:sigm}
\end{align}

**** Unbounded Tenure Function

Whenever a solution component $x_j$ is modified its tenure is
determined by the function in \ref{tenure.formula3}.

\begin{equation}
 \mathrm{tabu}_j(\tilde{p}_j(\mu)) = \left\{
\begin{array}{ll}
      T^{L}\frac{ 1-\tilde{p}_j(\mu))}{ \tilde{p}_j(\mu)}& x_j = x^{best}_j,\tilde{p}_j(\mu)\leq 0.5\\
        T^{L}\frac{ \tilde{p}_j(\mu))}{1- \tilde{p}_j(\mu)} & x_j = x^{best}_j,\tilde{p}_j(\mu)>0.5\\
      T^{L}  & x_j \neq x^{best}_j \\
\end{array} 
\right. \label{tenure.formula3}
\end{equation}

The plots of different tenure functions are shown in figure
\ref{tenure.functions}.


#+ATTR_LATEX: :placement [H]
#+ATTR_LATEX: :height 3.3in
#+CAPTION: Tenure as a function of approximation probabilities, $T^{U}=100$ and $T^{L}=10$.
#+NAME: tenure.functions
[[file:./figs/compareTenure.pdf]]

** Algorithm

Based on the previous discussion, we can provide a description of the
*Directional Tabu Algorithm* (DTA) (see Algorithm \ref{FigDTA}).

\begin{algorithm}
\caption{Directional Tabu Algorithm (general scheme)} \label{FigDTA}
\begin{algorithmic}[1]
\REQUIRE $\mu$ -- vector of temperature values, $K$ -- number of
temperature stages, $nfail^*$ -- restart parameter, $niters$ -- maximum
number of tabu search iterations, $d$ -- number of
iterations between memory updates.  
\ENSURE \STATE $x^{best}=$construct random solution;
  \WHILE {stopping criterion =  FALSE} \label{main.cycle.start} 
   \STATE $x =$ construct random solution 
   \STATE $x^{min}=x$ \STATE reset the long term memory: $H^1$, $H^0$, $f^1$, $f^0$ 
   \STATE update vectors $H^1$, $H^0$, $f^1$, $f^0$ using $x^{min}$ 
    \FOR {$nfail=0$ to $nfail^*$}\label{nfail.start}
      \STATE $x^{old}=x^{min}$ 
          \FOR{$k=0$ to $K$}        \label{temp.start}
            \STATE SearchProcedure($x,x^{min},H^1,H^0,f^1,f^0,niters,\mu_k,d$) [see Alg. \ref{FigTabu}]\label{temp.end}
         \ENDFOR
         \IF{$f(x^{old})>f(x^{min})$}
             \STATE $nfail=0$
         \ENDIF        
    \ENDFOR \label{chapter2:FigGES:nfail.end}
    \IF{$f(x^{best})>f(x^{min})$}
        \STATE $x^{best}=x^{min}$
    \ENDIF            
\ENDWHILE \label{main.cycle.end}
\RETURN $x^{best}$
\end{algorithmic}
\end{algorithm}

The presented pseudo-code describes the algorithm for solving
minimization problems similar to (\ref{general.model}). The main loop
(lines\ref{main.cycle.start}-\ref{main.cycle.end}) is repeated until
some stopping criteria is satisfied.  Within a temperature cycle
(lines\ref{temp.start}-\ref{temp.end}), we repeatedly launch a version
of a tabu search (line \ref{temp.end}) using an increasing sequence of
temperatures,  $\mu_1,\ldots\mu_k$. The long term memory captured in
vectors $H_0,H_1,f^1,f^0$ is constantly updated inside the search
procedure.  The temperature cycles are repeated until $nfail^*$
consecutive cycles without any improvement  (line \ref{nfail.start}).

\begin{algorithm}
\caption{Tabu Search Procedure} \label{FigTabu}
\begin{algorithmic}[1]
\REQUIRE $x$ -- current solution, $x^{min}$ -- current best solution, $H^1$, $H^0$  -- long term memory data [see (\ref{listH0}), (\ref{listH1})],  vectors $f^1, f^0$  [ $f^1_j$ ($f^0_j$) equals to the best found objective for the solution having $jth$ component equal to one (zero)], $\mu_k$ -- current temperature value, $niters$ -- maximum number of tabu iterations, $d$~-- number of iterations between memory updates.
    \STATE $\tilde{p}(\mu_k)$=\text{calculate probabilities}(${H^1},H^0,f^1,f^0, \mu_k$)$\quad\quad\quad\quad$\label{probability.generation} $\quad\quad$[see (\ref{approximation.probability})]    
    %\STATE $x_{best}=x$; $n=|x|$; $M=\{1,2,\ldots,n\}$; $step=0$;
    %$impr=$\bf{true}; $R=\emptyset$
    \STATE $lastChanged(j)=-\infty$; $tabu(j)=T^{L}$ for all $j$
    \FOR{$iter=1$ to $niters$}
            \STATE $TabuSet=\emptyset$ \label{init.tabu.set}
            \FOR{$y$ in $N(x)$}
                   \STATE $modInd = \{j: x_j\neq y_j\}$
                   \FOR{$j$ in $modInd$}
                        \IF{[$iter-lastChanged(j)<tabu(j)$] and [$f(y)\geq f(x_{min})$]} 
                        \STATE $TabuSet=TabuSet\cup y$
                        \ENDIF
                   \ENDFOR
            \ENDFOR
            \STATE $NonTabuSet= N(x)- TabuSet$
            \IF{$NonTabuSet\neq \emptyset$}
            \STATE $x^{new} = $ the best solution in $NonTabuSet$            
            \ELSE
            \STATE $x^{new} = $ the oldest tabu solution in $TabuSet$
            \ENDIF             \label{xnew.choosen}
            \STATE $modInd = \{j: x^{new}_j\neq x_j\}$ {\it \#only look at the components that changed}\label{who.changed}
            \FOR{$j$ in $modInd$}
            \IF{($x_j^{min}\neq x^{new}_j$)} 
            \STATE $tabu(j)=4(T^U-T^L) \tilde{p}_j(\mu)^2 -4(T^{U}-T^{L}) \tilde{p}_j(\mu)+T^{U} $  $\quad$[see (\ref{tenure.formula3})]
            \ELSE 
            \STATE $tabu(j)=T^{L}$ $\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad$[see (\ref{tenure.formula3})]\label{set.tabu2}
            \ENDIF
            \ENDFOR            
            \IF{[$iteration \bmod d = 0 $] OR [$f(x)<f(x^{min})$]} 
            \STATE update vectors $H^1$, $H^0$, $f^1$, $f^0$ using $x$             
            \ENDIF %    \ENDWHILE
            \STATE $x=x^{new}$
            \IF{[$f(x)<f(x^{min})$]} 
            \STATE $x^{min}=x$
            \ENDIF %    \ENDWHILE            
    \ENDFOR
    \RETURN $x,x^{min}$
\end{algorithmic}
\end{algorithm}


Our search procedure is similar to the tabu search method, but it also
includes a dynamic tabu tenure mechanism that uses long-term
memory(see Algorithm \ref{FigTabu}). At the beginning of the search
procedure, we calculate the approximation probabilities to Boltzmann's
distribution as defined in (\ref{approximation.probability}) using the
long-term memory data structures (Algorithm \ref{FigTabu},
line\ref{probability.generation}). The search procedure consists
of$niters$ iterations. At each iteration, all the solution in the
neighborhood of a current solution $x$ are split into two
sets$TabuSet$ and $NonTabuSet$. If $NonTabuSet$ is not empty, then the
best solution in this set becomes a new current solution, otherwise we
select the solution from $TabuSet$ that is closest to a non-tabu
status (Algorithm \ref{FigTabu},
lines\ref{init.tabu.set}-\ref{xnew.choosen}). After a new current
solution is chosen, we scan trough all the components that were
modified and set the prohibition duration $\mathrm{tabu}_j$ for each
of the corresponding components (Algorithm \ref{FigTabu},
lines\ref{who.changed}-\ref{set.tabu2}). We use the current solution
to update the memory structures involved in calculations of
probabilities $\bar{p}_j$ every $d$ iterations or if the current
solution $x$ is better than $x^{min}$.

* Computational Tests
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_FILE_NAME: computational.pdf
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:EXPORT_TITLE: 
:header-args: :session computational :tangle no :exports all :results output
:END:      

\label{comp.test}

** Experimental Setup for Pairwise Comparison
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_FILE_NAME: compare.pdf
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:EXPORT_TITLE: 
:header-args: :session compare :tangle "code/compare.py" :exports all :results output
:END:



For a pairwise comparison, we consider the framework developed by cite:shams2018 to analysis the proposed algorithm. We implemented performance probability and risk differences using bootstrap method. The performance probability measure the similarities between DTA and standard tabu and the risk differences contrasts these two algorithms. In the performance probability we calculate the probability of DTA obtains at least as good as standard tabu over time and in risk differences we compute the difference probability of DTA obtains strictly better solution than tabu, and the probability of that standard tabu obtains strictly better solution than DTA. Therefore, in performance probability growing the number to 1 shows a better performance and in risk differences taking distance from zero to 1 shows the priority. 
two algorithms $\mathcal A$ and
** Experiment Identifications and Parameters
\label{param.alg}

In order to study the performance of the proposed algorithm, the
directional tabu algorithm (DTA) is compared with standard tabu. Based on
experience, the most important parameters in DTA are number of
iterations, minimum tenure size, and maximum tenure size. In the other
hand, the number of iterations is the only factor which has an effect
on standard tabu. Different algorithms were designed and ran on test
problems. The settings for each algorithms are as follows.

The number of iterations for each loop of DTA is designed in
$100,000$, $300,000$, and $500,000$. The number of iterations for
standard Tabu is set to infinity and the algorithm will be stop on run
time criterion and is shown as Tabu experiment ID. The reason is
resetting the standard tabu on a fixed number of iterations make it
memoryless whereas the DTA improves the solution in each iterations by
keeping the memory. The minimum tenure size is set to 5 and 7 and the
maximum tenure size is set 80 and 120 for DTA. Different algorithms
and settings with the corresponding experience ID are shown in table
\ref{param.alg.table}.

\begin{table}[h]
\centering
\caption{Experiment IDs with its parameter setting}
\label{param.alg.table}
\begin{tabular}{l|l|l|l}
\hline
\textbf{No. of Iterations} & \textbf{Min Tenure} & \textbf{Max Tenure} & \textbf{Exp ID} \\ \hline
\multirow{4}{*}{100,000}   & \multirow{2}{*}{5}  & 80                  & DTA01           \\ \cline{3-4} 
                           &                     & 120                 & DTA02           \\ \cline{2-4} 
                           & \multirow{2}{*}{7}  & 80                  & DTA03           \\ \cline{3-4} 
                           &                     & 120                 & DTA04           \\ \hline
\multirow{4}{*}{300,000}   & \multirow{2}{*}{5}  & 80                  & DTA05           \\ \cline{3-4} 
                           &                     & 120                 & DTA06           \\ \cline{2-4} 
                           & \multirow{2}{*}{7}  & 80                  & DTA07           \\ \cline{3-4} 
                           &                     & 120                 & DTA08           \\ \hline
\multirow{4}{*}{500,000}   & \multirow{2}{*}{5}  & 80                  & DTA09           \\ \cline{3-4} 
                           &                     & 120                 & DTA10           \\ \cline{2-4} 
                           & \multirow{2}{*}{7}  & 80                  & DTA11           \\ \cline{3-4} 
                           &                     & 120                 & DTA12           \\ \hline
\end{tabular}
\end{table}

\subsection{Test Problems}
\label{test.problems}
Tabu algorithm can be applied in a different variety of problems. In a
special case, tabu algorithm shows a good performance in scheduling
problems in the literature. In order to evaluate the algorithm, we use
the Taillard's benchmark \citep{Taillard:1993} on job shop
scheduling. The local search is adapted from \citep{Grabowski:1986} on
both standard tabu and DTA.

Taillard's job shop test problems were randomly generated for
different sizes of machines and jobs. The number of machines varies
from 15 to 20 and the number of jobs from 15 to 100. They were
generated in different sizes as shown in table
\ref{table.taillards}. For each problems size, 10 random test problems
were generated.


\begin{table}[h]
\centering
\caption{Problems Size for Taillard's Job Shop Benchmark}
\label{table.taillards}
\begin{tabular}{l|l|l}
\hline
\textbf{No. of Machines} & \textbf{No . of Jobs} & \textbf{Problem Size} \\ \hline
15                       & 15                    & Problems 1             \\ \hline
15                       & 20                    & Problems 2             \\ \hline
20                       & 20                    & Problems 3             \\ \hline
15                       & 30                    & Problems 4             \\ \hline
20                       & 30                    & Problems 5             \\ \hline
15                       & 50                    & Problems 6             \\ \hline
20                       & 50                    & Problems 7             \\ \hline
20                       & 100                   & Problems 8             \\ \hline
\end{tabular}
\end{table}

** Computations
\label{computations}
All test problems are ran by all experiment ID algorithms using Newton
high performance computing (HPC) program at the University of
Tennessee \citep{NewtonHPC}. Each test problem ran ten times to reduce
random initial solution effect. The stopping criterion was 30
minutes.

Performance probability and risk difference which are discussed before
in \ref{performance.prob} and \ref{risk.diff} respectively, are
applied to evaluate the DTA performance. They are also plotted in 2-D
plot in which /x/-axis is the running time and /y/-axis is
probabilities for performance probability plot and portions for risk
difference plot. The plots also include appropriate confidence
interval. For the probability plot, when the probabilities and confidence
interval are close to 1, it shows that the proposed algorithm
performed better than tabu on the test problem. Similarly for risk
difference plot, when the proportions and confidence interval are
positive it means that the proposed algorithm is better than tabu on test
problems, when they are negative it means the proposed algorithm is
worse than tabu on test problems and if they are close to 0 it means
there is no meaningful difference between tabu and DTA on test
problems.

Among all experiment IDs in table \ref{param.alg.table}, it has been
observed that the experiment ID DTA04 performed better than the others
on the test problems. It has been also observed that both DTA and tabu
algorithm perform similarly on test problems in problem sizes problem
1, problem 6, problem 7, and problem 8 in table
\ref{table.taillards} because they are easy to solve. Therefore, we
compared the DTA04 with tabu algorithm on problem sizes problem 2,
problem 3, problem 4, and problem 5in table \ref{table.taillards}.

Since the experiment ID DTA04 outperformed other experiment IDs, the
standard tabu algorithm is only compared with DTA04 in detail. The
performance probability plot for DTA04 compared to standard tabu on all
problems sizes 2, 3, 4, and 5 samples is shown in figure
\ref{fig.DTA04}. Since the sample size is large enough, the confidence
interval method is adjusted Wald and the confidence nominal level is
95%. In order to see the performance in detail, the performance
probability plot for each problems size is shown separately in figure
\ref{fig.DTA04.4p}. In this plot, we applied Wilson's score confidence
interval with 95% nominal level because this method has a good
coverage probability for any size of sample. As we can see in figures
\ref{fig.DTA04} and \ref{fig.DTA04.4p}, the DTA outperformed
significantly on all problems sizes significantly.

#+ATTR_LATEX: :placement [H]
#+ATTR_LATEX: :height 3.3in
#+CAPTION: Performance probability plot for DTA04 compared to tabu on problems sizes 2, 3, 4, and 5.
#+NAME: fig.DTA04
[[file:./figs/comparenotworse-all-ref2016T04-2016C04-Adjusted-Wald-Interval.png]]

#+ATTR_LATEX: :placement [H]
#+ATTR_LATEX: :height 2.7in
#+CAPTION: Performance probability plot for DTA04 compared to tabu for each problem size.
#+NAME: fig.DTA04.4p
[[file:./figs/comparenotworse-detail-ref2016T04-2016C04-Wilsons-Score.png]]

The risk difference plot for DTA04 is necessary to show how much this
algorithm is better than tabu algorithm. Therefore, the risk
difference plot over all problem sizes 2, 3, 4, and 5 is shown in
figure \ref{fig.RD.DTA04}. The detail plots for problem sizes 1, 2, 3,
and 4 are shown separately in figure \ref{fig.RD.DTA04.4p}. The
confidence intervals with confidence nominal level 95% in both figures
\ref{fig.RD.DTA04} and \ref{fig.RD.DTA04.4p} are calculated by
Wilson's score with continuity correction, because this method has the
best performance among all others. We can see that the DTA04 has a
significant superiority in performance compared to tabu algorithm.

#+ATTR_LATEX: :placement [H]
#+ATTR_LATEX: :height 3.3in
#+CAPTION: Risk difference plot for DTA04 compared to tabu on problems sizes 2, 3, 4, and 5.
#+NAME: fig.DTA04
[[file:./figs/comparediff-all-ref2016T04-2016C04-Wilson-w-Cont.png]]

#+ATTR_LATEX: :placement [H]
#+ATTR_LATEX: :height 2.7in
#+CAPTION: Risk difference plot for DTA04 compared to tabu for each problem size.
#+NAME: fig.DTA04.4p
[[file:./figs/comparediff-detail-ref2016T04-2016C04-Wilson-w-Cont.png]]

* Conclusions

The proposed logistic regression model achieves high accuracy for the tabu algorithm on the considered class of job shop scheduling problems. The optimal parameter $\theta$ changes with respect to the time used to collect the information, but the range of the optimal values is stable across different problem instances. Hence, it is possible to find an interval $[\theta_{min},\theta_{max}]$ that contains all optimal values for a given set of problem instances and time threshold $t$. Our working hypothesis is that this interval would provide a good estimate for the location of optimal $\theta$ values for the problem instance that were not used to train the model. In some sense, the interval $[\theta_{min},\theta_{max}]$ provides a prediction for what is optimal for a class of scheduling problems. 

Importantly, in order to find an optimal value of $\theta$ for a given problem, one needs to know an optimal solution of that problem. Clearly, when considering an optimization problem, which has not been previously solved, its optimal solution is not available. However, we can use the range $[\theta_{min},\theta_{max}]$ estimated from the testing problems as a predictor for the location of the optimal logistic parameter of the new problem instance. Given a grid of points from the interval $[\theta_{min},\theta_{max}]$, one of the points will be close to the optimal value. This property can be used to develop an algorithm that can use the logistic model to guide the search process. 

In this research, we have studied the use of learning models inside approximation algorithms for discrete optimization problems in which the algorithms are able to learn from the patterns in the search space. This learning improved the performance of algorithms during the process and there is no need to train the algorithm offline. Recent research has shown that training optimization algorithms can improve the performance significantly. Our contribution to this field cover the development in learning models in a way that no upfront learning is required for the algorithms. We provide the logistic regression learning model and also designed a directional tabu algorithm based on this idea in which a parameter of the algorithm is tuned during the process. We implemented the DTA on a benchmark of job shop scheduling problems to compare the performance with standard tabu algorithm. We compare the proposed DTA with standard tabu and it showed a good performance on job shop scheduling problems benchmark.


\bibliographystyle{apalike}

\bibliography{overallliterature.bib}


#  LocalWords:  Tabu minima tabu multi relinking Boltzmann's Sigmoid
#  LocalWords:  iteratively Shylo num usepackage onehalfspacing SRC
* DONE [#A] Description of the Database                      :hesam:NOEXPORT:
- Old Database schema: CREATE TABLE JOBSHOP(ID INTEGER PRIMARY KEY,EXP_ID
  TEXT NOT NULL,ALGO_NAME TEXT NOT NULL,PROBLEM_NAME TEXT NOT
  NULL,RUN_ID INTEGER NOT NULL,EPOCH_ID INTEGER NOT NULL,NO_ITER
  INTEGER ,TIME REAL NOT NULL,MAKESPAN REAL NOT NULL);
- New Database schema: create table records (algo TEXT, expid TEXT,
  problem TEXT, runid INTEGER, seed INTEGER, makespan INTEGER, time
  INTEGER, PRIMARY KEY (algo, expid, seed,makespan,time));
- for "problem" column in new version and "PROBLEM_NAME" in old
  version, we have two formats: "taxx.txt" and "jsp_problems/taxx.txt". In
  examples 1-3, the "taxx.txt" format is used and in examples 4-7, the
  "jsp_problems/taxx.txt" is used.

* DONE [#A] Description of the Data                          :hesam:NOEXPORT:
|---------+-----------------------------------------------------------------------|
| exp_id  | description                                                           |
|---------+-----------------------------------------------------------------------|
| 2016C01 | DTA (number of iterations: 100,000 + min tenure: 5 + max tenure: 80)  |
| 2016C05 | DTA (number of iterations: 300,000 + min tenure: 5 + max tenure: 80)  |
| 2016C09 | DTA (number of iterations: 500,000 + min tenure: 5 + max tenure: 80)  |
| 2016C03 | DTA (number of iterations: 100,000 + min tenure: 7 + max tenure: 80)  |
| 2016C04 | DTA (number of iterations: 100,000 + min tenure: 7 + max tenure: 120) |
| 2016T03 | Standard Tabu (number of iterations: 500,000)                         |
| 2016T01 | Standard Tabu (number of iterations: 100,000)                         |
| 2016T02 | Standard Tabu (number of iterations: 300,000)                         |
| 2016C06 | DTA (number of iterations: 300,000 + min tenure: 5 + max tenure: 120) |
| 2016C10 | DTA (number of iterations: 500,000 + min tenure: 5 + max tenure: 120) |
| 2016C07 | DTA (number of iterations: 300,000 + min tenure: 7 + max tenure: 80)  |
| 2016C08 | DTA (number of iterations: 300,000 + min tenure: 7 + max tenure: 120) |
| 2016C11 | DTA (number of iterations: 500,000 + min tenure: 7 + max tenure: 80)  |
| 2016C12 | DTA (number of iterations: 500,000 + min tenure: 7 + max tenure: 120) |
| 2016C02 | DTA (number of iterations: 100,000 + min tenure: 5 + max tenure: 120) |
| TEST50  | DTA (number of iterations: 50,000 + min tenure: 7 + max tenure: 120)  |
| TABU50  | Standard Tabu (number of iterations: 50,000)                          |
| RND01   | Random Solution Generation                                            |
| RND02   | Random Solution Generation                                            |
| 2016T04 | Standard Tabu (number of iterations: 2,000,000,000)                   |
|---------+-----------------------------------------------------------------------|

* Python Library for Generating Tables/Images                      :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle "code/libGenerateTablesImages.py" :exports all :results output
:END:      

** TODO set all system paths
#+BEGIN_SRC python
  import os
  import sqlite3
  import pandas as pd
  import time
  import numpy as np
  import matplotlib.pyplot as plt
  from matplotlib.pyplot import * 
  import scipy
  import random
  import scipy.stats as st
  from math import *
  # We need to use equation solver for some confidence interval methods
  # use the following command to install the library
  # sudo pip install sympy
  from sympy import solve, Symbol, Abs
  import sympy

  print(os.uname())

  if "oshylo" in os.uname()[1]: db_path = "/home/oshylo/olegio@gmail.com/benchmarks/results-JSP-2017/dataJobshop.db" 
  elif "lepidoptera" in os.uname()[1]: db_path = "/home/quasiquasar/olegio@gmail.com/benchmarks/results-JSP-2017/dataJobshop.db"  
  elif "Eta" in os.uname()[1]: db_path = "/home/hesam/Database/dataJobshop.db"
  else: db_path = "dataJobshop.db"

  if "oshylo" in os.uname()[1]: benchmark_path = "/home/oshylo/olegio@gmail.com/benchmarks/"
  elif "lepidoptera" in os.uname()[1]: benchmark_path = "/home/quasiquasar/olegio@gmail.com/benchmarks/"
  elif "Eta" in os.uname()[1]: benchmark_path = "/home/hesam/Database/JSBenchmark"
  else: benchmark_path = "./"


  print(db_path)
  print(benchmark_path)
#+END_SRC

#+RESULTS:
: 
: >>> >>> >>> >>> >>> >>> ... ... ... >>> >>> >>> ('Linux', 'Eta', '4.4.0-97-generic', '#120-Ubuntu SMP Tue Sep 19 17:28:18 UTC 2017', 'x86_64')
: >>> ... ... ... ... >>> ... ... ... ... >>> >>> /home/hesam/Database/dataJobshop.db
: /home/hesam/Database/JSBenchmark

** function that reads data from DB

#+BEGIN_SRC python
  def readDBResults(problemsetnames, expids, benchmark_path, db_path):
     # set selected sql string        
     query = "SELECT * FROM records WHERE problem IN (%s) and expid IN (%s);" % (','.join('?' * len(problemsetnames)),','.join('?' * len(expids)))
     df=pd.DataFrame()
     # Read sqlite query results into a pandas DataFrame
     con = sqlite3.connect(db_path)
     df = pd.read_sql_query(query, con, params = problemsetnames+expids)
     con.close()
     return df  
#+END_SRC

** function that generates comparison data
#+BEGIN_SRC python
  def compareABSingleProblem(dataset, algoA, algoB, problem, timelimit):
  #     """
  #     The function pulls subset of data for a single problem, and compares two vectors (down-sizing if necessary).
  #     It returns a dictionary of counts 'nbetter', 'nsame', 'nworse' and 'total'
    
  #     \par dataset -- pandas dataframe with the data for a set of problems, and algorithms. 
  #     \par algoA -- string with a name of experiment id 
  #     \par algoB -- string with a name of experiment id 
  #     \par problem -- problem used for comparison
  #     \par timelimit -- cut the data with values in column time above this value
  #     """
     sbs = dataset[ (dataset.expid == algoA) & (dataset.problem==problem) & (dataset.time<=timelimit)][["seed","makespan"]]
     bestobjectivesA = list(sbs.groupby("seed").min()["makespan"])
     sbs = dataset[ (dataset.expid == algoB) & (dataset.problem==problem) & (dataset.time<=timelimit)][["seed","makespan"]]
     bestobjectivesB = list(sbs.groupby("seed").min()["makespan"])
     # downsample if necessary
     # I don't believ that just one missing equaty symbol ruined my code (((
     if len(bestobjectivesA)>=len(bestobjectivesB): 
        bestobjectivesA = random.sample(bestobjectivesA, len(bestobjectivesB))
     if len(bestobjectivesB)>len(bestobjectivesA): 
        bestobjectivesB = random.sample(bestobjectivesB, len(bestobjectivesA))
     # calculate beta parameters
     nbetter = 0 
     nsame = 0
     nworse = 0 
     for j in range(0,len(bestobjectivesB)):
        o1 = bestobjectivesA[j]
        o2 = bestobjectivesB[j]
        if    o1<o2: nbetter=nbetter+1 
        elif o1==o2: nsame=nsame+1
        else: nworse=nworse+1
     return {'nbetter': nbetter, 'nsame': nsame, 'nworse': nworse,'total': len(bestobjectivesA)}
#+END_SRC

** function that generates comparison data based on bootstrap
#+BEGIN_SRC python
def compareABSingleBootstrap(dataset, algoA, algoB, problem, timelimit):
    """
    The function pulls subset of data for a single problem, and compares two vectors (down-sizing based on bootstrap parameter).
    It returns a dictionary of counts 'nbetter', 'nsame', 'nworse' and 'total'
  
    \par dataset -- pandas dataframe with the data for a set of problems, and algorithms. 
    \par algoA -- string with a name of experiment id 
    \par algoB -- string with a name of experiment id 
    \par problem -- problem used for comparison
    \par timelimit -- cut the data with values in column time above this value
    """
    sbs = dataset[ (dataset.expid == algoA) & (dataset.problem==problem) & (dataset.time<=timelimit)][["seed","makespan"]]
    bestobjectivesA = list(sbs.groupby("seed").min()["makespan"])
    sbs = dataset[ (dataset.expid == algoB) & (dataset.problem==problem) & (dataset.time<=timelimit)][["seed","makespan"]]
    bestobjectivesB = list(sbs.groupby("seed").min()["makespan"])

    # downsample based on percent of sizes with replacement
    bestobjectivesA = np.random.choice(bestobjectivesA, len(bestobjectivesA),replace=True)
    bestobjectivesB = np.random.choice(bestobjectivesB, len(bestobjectivesB),replace=True)

    # calculate beta parameters
    nbetter = 0 
    nsame = 0
    nworse = 0 
    for o1 in bestobjectivesA:
       for o2 in bestobjectivesB:
          if    o1<o2: nbetter=nbetter+1 
          elif o1==o2: nsame=nsame+1
          else: nworse=nworse+1
    return {'nbetter': nbetter, 'nsame': nsame, 'nworse': nworse,'total': len(bestobjectivesA)*len(bestobjectivesB)}
#+END_SRC
** [BOOSTING] function to generate data for bootstrap from pandas and return as array
#+BEGIN_SRC python
def genABobj(dataset, algoA, algoB, problem, timelimit):
    """
    The function pulls subset of data for a single problem, and compares two vectors (down-sizing based on bootstrap parameter).
    It returns a dictionary of counts 'nbetter', 'nsame', 'nworse' and 'total'
  
    \par dataset -- pandas dataframe with the data for a set of problems, and algorithms. 
    \par algoA -- string with a name of experiment id 
    \par algoB -- string with a name of experiment id 
    \par problem -- problem used for comparison
    \par timelimit -- cut the data with values in column time above this value
    """
    sbs = dataset[ (dataset.expid == algoA) & (dataset.problem==problem) & (dataset.time<=timelimit)][["seed","makespan"]]
    bestobjectivesA = list(sbs.groupby("seed").min()["makespan"])
    sbs = dataset[ (dataset.expid == algoB) & (dataset.problem==problem) & (dataset.time<=timelimit)][["seed","makespan"]]
    bestobjectivesB = list(sbs.groupby("seed").min()["makespan"])
    return {'objA': bestobjectivesA, 'objB': bestobjectivesB}
#+END_SRC
** [BOOSTING] function to compare to sets of arrays
#+BEGIN_SRC python
def compareABarr(bestobjectivesA, bestobjectivesB):
    # downsample based on percent of sizes with replacement
    bestobjectivesA = np.random.choice(bestobjectivesA, len(bestobjectivesA),replace=True)
    bestobjectivesB = np.random.choice(bestobjectivesB, len(bestobjectivesB),replace=True)

    # calculate beta parameters
    nbetter = 0 
    nsame = 0
    nworse = 0 
    for o1 in bestobjectivesA:
       for o2 in bestobjectivesB:
          if    o1<o2: nbetter=nbetter+1 
          elif o1==o2: nsame=nsame+1
          else: nworse=nworse+1
    return {'nbetter': nbetter, 'nsame': nsame, 'nworse': nworse,'total': len(bestobjectivesA)*len(bestobjectivesB)}
#+END_SRC
** get information about all runs on a problem
#+BEGIN_SRC python
  def statsOnSingleProblem(dataset, algoA, algoB, problem, timelimit):
     """
     The function pulls subset of data for a single problem from two expid's algoA and algoB.
     It prints best objectives of both expid's.
   
     \par dataset -- pandas dataframe with the data for a set of problems, and algorithms. 
     \par algoA -- string with a name of experiment id 
     \par algoB -- string with a name of experiment id 
     \par problem -- problem used for comparison
     \par timelimit -- cut the data with values in column time above this value
     """
     sbs = dataset[ (dataset.expid == algoA) & (dataset.problem==problem) & (dataset.time<=timelimit)][["seed","makespan"]]
     bestobjectivesA = list(sbs.groupby("seed").min()["makespan"])
     sbs = dataset[ (dataset.expid == algoB) & (dataset.problem==problem) & (dataset.time<=timelimit)][["seed","makespan"]]
     bestobjectivesB = list(sbs.groupby("seed").min()["makespan"])
     print(bestobjectivesA)
     print(bestobjectivesB)
#+END_SRC

** confidence interval for performance probability (binomial proportion)

#+BEGIN_SRC python
def WaldInterval(temp_count, temp_enum, conf_int):
    # The Wald interval method based on normal theory approximation
    z = st.norm.ppf(conf_int)
    conf_min = 1.0*temp_count/temp_enum - (z)*sqrt((1.0*temp_count/temp_enum)*(temp_enum-temp_count)/temp_enum)
    conf_max = 1.0*temp_count/temp_enum + (z)*sqrt((1.0*temp_count/temp_enum)*(temp_enum-temp_count)/temp_enum)
    temp_title = "Wald-Interval"
    return(conf_min, conf_max, temp_title)

def CPInterval(temp_count, temp_enum, conf_int):
   # The Clopper-Pearson interval method in order to avoid normal approximation (exact method)
   F1 = st.f.ppf(conf_int, 2*temp_enum-2*temp_count+2, 2*temp_count)
   F2 = st.f.ppf(conf_int, 2*temp_count, 2*temp_enum-2*temp_count)
   conf_min = 1.0*temp_count/(temp_count + (temp_enum - temp_count + 1)*F1)
   conf_max = 1.0*((temp_count + 1)*F2) / (temp_enum - temp_count + (temp_count + 1)*F2)
   temp_title = "Clopper-Pearson-Interval"
   return(conf_min, conf_max, temp_title)

def AdjustWald(temp_count, temp_enum, conf_int):
   # The adjusted Wald interval method based on normal approximation with a slight modification
   z = st.norm.ppf(conf_int)
   pi_w = 1.0*(temp_count+2)/(temp_enum+4)
   s2_w = pi_w*(1-pi_w)/(temp_enum+4)
   conf_min = pi_w - z*sqrt(s2_w)
   conf_max = pi_w + z*sqrt(s2_w)
   temp_title = "Adjusted-Wald-Interval"
   return(conf_min, conf_max, temp_title)

def WilsonScore(temp_count, temp_enum, conf_int):
   # The Wilson's score interval method is the inversion of the score test
   z = st.norm.ppf(conf_int)
   pi_w = 1.0*temp_count/temp_enum
   conf_min = (pi_w + z**2/(2*temp_enum) - z*sqrt(pi_w*(1-pi_w)/temp_enum + z**2/(4*temp_enum**2)))/ (1+z**2/temp_enum)
   conf_max = (pi_w + z**2/(2*temp_enum) + z*sqrt(pi_w*(1-pi_w)/temp_enum + z**2/(4*temp_enum**2)))/ (1+z**2/temp_enum)
   temp_title = "Wilsons-Score"
   return(conf_min, conf_max, temp_title)
#+END_SRC

** confidence interval for difference probability (between two binomial proportions)
#+BEGIN_SRC python
def AsymptoticC(nbetter, nworse, nsame, ntotal, conf_int):
    z = st.norm.ppf(conf_int)
    se = sqrt(nbetter + nworse - (1.0*(nbetter - nworse)**2/ntotal))/ntotal
    theta_hat = 1.0*(nbetter - nworse)/ntotal
    conf_min = theta_hat - (z*se + 1.0*(1/ntotal))
    conf_max = theta_hat + (z*se + 1.0*(1/ntotal))
    temp_title = "Asymptotic-w-Cont"
    return(conf_min, conf_max, temp_title)

def AsymptoticNC(nbetter, nworse, nsame, ntotal, conf_int):
    z = st.norm.ppf(conf_int)
    se = sqrt(nbetter + nworse - (1.0*(nbetter - nworse)**2/ntotal))/ntotal
    theta_hat = 1.0*(nbetter - nworse)/ntotal
    conf_min = theta_hat - (z*se)
    conf_max = theta_hat + (z*se)
    temp_title = "Asymptotic-wo-Cont"
    return(conf_min, conf_max, temp_title)

def WilsonNC(nbetter, nworse, nsame, ntotal, conf_int):
    nsame_n = 0
    z = st.norm.ppf(conf_int)
    x = Symbol('x', real=True)
    (l2, u2) = solve(Abs(x - 1.0*(nsame+nbetter)/ntotal) - z*sympy.sqrt(1.0*x*(1-x)/ntotal), x)
    (l3, u3) = solve(Abs(x - 1.0*(nsame+nworse)/ntotal) - z*sympy.sqrt(1.0*x*(1-x)/ntotal), x)
    dl2 = 1.0*(nsame+nbetter)/ntotal -l2
    du2 = u2 - 1.0*(nsame+nbetter)/ntotal
    dl3 = 1.0*(nsame+nworse)/ntotal - l3
    du3 = u3 - 1.0*(nsame+nworse)/ntotal
    if (((nsame+nbetter)==0) or ((nworse+nsame_n)==0) or ((nsame+nworse)==0) or ((nbetter+nsame_n)==0)):
        phi_h = 0
    else:
        temp = sqrt((nsame+nbetter)*(nworse+nsame_n)*(nsame+nworse)*(nbetter+nsame_n))
        phi_h = 1.0*(nsame*nsame_n-nbetter*nworse)/temp
    delta = sqrt(dl2**2 - 2*phi_h*dl2*du3 + du3**2)
    epsil = sqrt(du2**2 - 2*phi_h*du2*dl3 + dl3**2)
    theta_h = 1.0*(nbetter-nworse)/ntotal
    conf_min = theta_h - delta
    conf_max = theta_h + epsil
    temp_title = "Wilson-wo-Cont"
    return(conf_min, conf_max, temp_title)

def WilsonC(nbetter, nworse, nsame, ntotal, conf_int):
    nsame_n = 0
    z = st.norm.ppf(conf_int)
    x = Symbol('x', real=True)
    (l2, u2) = solve(Abs(x - 1.0*(nsame+nbetter)/ntotal) - z*sympy.sqrt(1.0*x*(1-x)/ntotal) - 1.0*1/(2*ntotal), x)
    (l3, u3) = solve(Abs(x - 1.0*(nsame+nworse)/ntotal) - z*sympy.sqrt(1.0*x*(1-x)/ntotal) - 1.0*1/(2*ntotal), x)
    dl2 = 1.0*(nsame+nbetter)/ntotal -l2
    du2 = u2 - 1.0*(nsame+nbetter)/ntotal
    dl3 = 1.0*(nsame+nworse)/ntotal - l3
    du3 = u3 - 1.0*(nsame+nworse)/ntotal
    if (((nsame+nbetter)==0) or ((nworse+nsame_n)==0) or ((nsame+nworse)==0) or ((nbetter+nsame_n)==0)):
        phi_h = 0
    else:
        temp = sqrt((nsame+nbetter)*(nworse+nsame_n)*(nsame+nworse)*(nbetter+nsame_n))
        phi_h = 1.0*(nsame*nsame_n-nbetter*nworse)/temp
    delta = sqrt(dl2**2 - 2*phi_h*dl2*du3 + du3**2)
    epsil = sqrt(du2**2 - 2*phi_h*du2*dl3 + dl3**2)
    theta_h = 1.0*(nbetter-nworse)/ntotal
    conf_min = theta_h - delta
    conf_max = theta_h + epsil
    temp_title = "Wilson-w-Cont"
    return(conf_min, conf_max, temp_title)
#+END_SRC

** main 
#+BEGIN_SRC python
  def main():
    print("..in main..")

  if __name__ == "__main__":
      main() 
#+END_SRC

* Example: Python Library for Generating Tables/Images             :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle no :exports all :results output
:END:      


** pull out data for some experiment id on a given set of problems

#+BEGIN_SRC python
import sys 
import random 
sys.path.append('./code')
from libGenerateTablesImages  import *
# working with Taillard instances

startid = 11
endid = 11


problemsetids = ["%.2d" % i for i in range(startid,endid+1)]
problemsetnames = ['ta'+str(i)+'.txt' for i in problemsetids]

# names of exp_ids to compare
algoA = 'tmin05max20temp'
algoB = 'tmin05max20ch2'

print(benchmark_path)
print(db_path)
# get all relevant data
dataset = readDBResults(problemsetnames, [algoA,algoB], benchmark_path, db_path)

# get stats on #better,#worse,#same,#total
# save results in a list of dictionaries
# each dictionary key is a tuple (problem, time) and return stats
# for a specific problem and time threshold
stats = {}
for minute in range(1,31):
    timelimit = minute*60
    for problem in problemsetnames:
        stats[(problem,timelimit)]=compareABSingleProblem(dataset, algoA, algoB, problem, timelimit)
    print(stats[(problem,timelimit)])
#+END_SRC

notes: 
- ch1 remove +1 for f1min==f0min
- ch2 remove temperatures 
- ch3 change mu = mu+delta
- ch4 change tenuremax form 20 to 30 

** check unique problem names
#+BEGIN_SRC python
np.unique(dataset[ dataset.expid==algoA].problem)
#+END_SRC

** plot pairwise comparison
#+BEGIN_SRC python
  fig, axs = plt.subplots(5,2, figsize=(15, 6), facecolor='w', edgecolor='k')
  fig.subplots_adjust(hspace = 0.7, wspace=0.11)
  axs = axs.ravel()

  for idx,problem in enumerate(problemsetnames):
     temp_time = []
     temp_prob1 = []
     temp_prob2 = []
     temp_prob3 = []
     for minute in range(1,31):
        temp_time.append(minute)
        # plot probability of better or equal
        probbetter = 0 
        probsame = 0
        probworse = 0 
        # if there are some records (avoid division by zero)
        if stats[(problem,minute*60)]['total']>0:
           probbetter = 1.0*(stats[(problem,minute*60)]['nbetter'] )/stats[(problem,minute*60)]['total']
           probsame = 1.0*(stats[(problem,minute*60)]['nsame'] )/stats[(problem,minute*60)]['total']        
           probworse = 1.0*(stats[(problem,minute*60)]['nworse'] )/stats[(problem,minute*60)]['total']        
        else: print("either "+algoA+" or "+algoB+" lack data for problem "+problem)
        temp_prob1.append(probbetter)    
        temp_prob2.append(probsame)    
        temp_prob3.append(probworse)    
     # now plot
     _=axs[idx].plot(temp_time, temp_prob1, 'b-',label=algoA+'-better-than-'+algoB)
     _=axs[idx].plot(temp_time, temp_prob2, 'g*-',label=algoA+'-same-as-'+algoB)
     _=axs[idx].plot(temp_time, temp_prob3, 'r--',label=algoA+'-worse-than-'+algoB)
     _=axs[idx].yaxis.set_ticks([0,0.5,1.0])
     _=axs[idx].axis([0, 31, -0.1, 1.1])
     # plot axis labels, but not everywhere
     if idx>7: _=axs[idx].set_xlabel("time")
     if idx==4: _=axs[idx].set_ylabel("probability")
     _=axs[idx].set_title(problem[13:17],fontsize=10)  
     if idx==0: _=axs[idx].legend(bbox_to_anchor=(0., 1.12, 2.11, 1.12), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)

  #plt.show()
  #plt.close()
  plt.savefig("figs/compare-"+algoA+"-"+algoB+"-ta"+str(startid)+"-"+str(endid)+".png")  
  plt.close()  
#+END_SRC

** pull out data for some experiment id on a single problem

#+BEGIN_SRC python
  fig, axs = plt.subplots(1,1, figsize=(15, 6), facecolor='w', edgecolor='k')
  fig.subplots_adjust(hspace = 0.7, wspace=0.11)
  #axs = axs.ravel()

  for idx,problem in enumerate(problemsetnames):
     temp_time = []
     temp_prob1 = []
     temp_prob2 = []
     temp_prob3 = []
     for minute in range(1,31):
        temp_time.append(minute)
        # plot probability of better or equal
        probbetter = 0 
        probsame = 0
        probworse = 0 
        # if there are some records (avoid division by zero)
        if stats[(problem,minute*60)]['total']>0:
           probbetter = 1.0*(stats[(problem,minute*60)]['nbetter'] )/stats[(problem,minute*60)]['total']
           probsame = 1.0*(stats[(problem,minute*60)]['nsame'] )/stats[(problem,minute*60)]['total']        
           probworse = 1.0*(stats[(problem,minute*60)]['nworse'] )/stats[(problem,minute*60)]['total']        
        else: print("either "+algoA+" or "+algoB+" lack data for problem "+problem)
        temp_prob1.append(probbetter)    
        temp_prob2.append(probsame)    
        temp_prob3.append(probworse)    
     # now plot
     _=axs.plot(temp_time, temp_prob1, 'b-',label=algoA+'-better-than-'+algoB)
     _=axs.plot(temp_time, temp_prob2, 'g*-',label=algoA+'-same-as-'+algoB)
     _=axs.plot(temp_time, temp_prob3, 'r--',label=algoA+'-worse-than-'+algoB)
     _=axs.yaxis.set_ticks([0,0.5,1.0])
     _=axs.axis([0, 31, -0.1, 1.1])
     # plot axis labels, but not everywhere
     _=axs.set_xlabel("time")
     _=axs.set_ylabel("probability")
     _=axs.set_title(problem[13:17],fontsize=10)  
     _=axs.legend(bbox_to_anchor=(0., 1.0, 1, 1.0), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)

  #plt.show()
  #plt.close()
  plt.savefig("figs/SINGLE-compare-"+algoA+"-"+algoB+"-ta"+str(startid)+"-"+str(endid)+".png")  
  plt.close()  
#+END_SRC

** look at a single problem stats
#+BEGIN_SRC python
statsOnSingleProblem(dataset, algoA, algoB, 'ta44.txt', 30*60)
#+END_SRC

** check all best solutions
#+BEGIN_SRC python
import sys 
import random 
sys.path.append('./code')
from libGenerateTablesImages  import *
# working with Taillard instances
problemsetids = ["%.2d" % i for i in range(11,21)]
problemsetnames = ['ta'+str(i)+'.txt' for i in problemsetids]


query = "SELECT * FROM records"
df=pd.DataFrame()
# Read sqlite query results into a pandas DataFrame
con = sqlite3.connect(db_path)
df = pd.read_sql_query(query, con)
con.close()
df.shape
#+END_SRC


#+BEGIN_SRC python
bestobjectivesB = df.groupby("problem").min()["makespan"]
bestobjectivesB
#+END_SRC

* Example2: Combine Stats for Different Problem Classes (two algorithms) :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle no :exports all :results output
:END:



#+BEGIN_SRC python
import sys 
import random 
sys.path.append('./code')
from libGenerateTablesImages  import *

# working with Taillard instances
# define 4 classes of problems ta11-ta20, ta21-30,ta31-40,ta41-50
problemclass={0: [11,21],1:[21,31],2:[31,41],3:[41,51]}

fig, axs = plt.subplots(2,2, figsize=(15, 6), facecolor='w', edgecolor='k')
fig.subplots_adjust(hspace = 0.3, wspace=0.11)
axs = axs.ravel()
# names of exp_ids to compare
algoA = 'tenure05'
algoB = 'tenure03'

for cl in range(4):
   startid = problemclass[cl][0]
   endid = problemclass[cl][1]
   print("reading problems ta"+str(startid)+" to "+str(endid))
   problemsetnames = ['ta'+str(i)+'.txt' for i in range(startid,endid)]
   # get all relevant data
   dataset = readDBResults(problemsetnames, [algoA,algoB], benchmark_path, db_path)
   temp_time = []
   
   temp_prob1 = []
   temp_prob2 = []
   temp_prob3 = []
   # get stats on #better,#worse,#same,#total
   # save results in a list of dictionaries
   # each dictionary key is a tuple (problem, time) and return stats
   # for a specific problem and time threshold
   stats = {}
   # look at the first 30 minutes in the data (range(1,31))
   for minute in range(1,31):
      # timelimit in seconds
      timelimit = minute*60
      pbetter = 0
      psame = 0
      pworse = 0 
      for problem in problemsetnames:
          stats[(problem,timelimit)]=compareABSingleProblem(dataset, algoA, algoB, problem, timelimit)
          if stats[(problem,minute*60)]['total']>0:
             pbetter = pbetter + 1.0*(stats[(problem,minute*60)]['nbetter'] )/stats[(problem,minute*60)]['total']
             psame = psame + 1.0*(stats[(problem,minute*60)]['nsame'] )/stats[(problem,minute*60)]['total']        
             pworse = pworse + 1.0*(stats[(problem,minute*60)]['nworse'] )/stats[(problem,minute*60)]['total']        
          else: print("either "+algoA+" or "+algoB+" lack data for problem "+problem)

      temp_time.append(minute)
      temp_prob1.append(pbetter/len(problemsetnames))    
      temp_prob2.append(psame/len(problemsetnames))    
      temp_prob3.append(pworse/len(problemsetnames))          
      
   # now plot
   # probability better
   _=axs[cl].plot(temp_time, temp_prob1, 'b-',label=algoA+'-better-than-'+algoB)
   #probability same
   _=axs[cl].plot(temp_time, temp_prob2, 'g*-',label=algoA+'-same-as-'+algoB)
   # probability worse
   _=axs[cl].plot(temp_time, temp_prob3, 'r--',label=algoA+'-worse-than-'+algoB)
   _=axs[cl].yaxis.set_ticks([0,0.5,1.0])
   _=axs[cl].axis([0, 31, -0.1, 1.1])
   # plot axis labels but not everywhere
   if cl>1: _=axs[cl].set_xlabel("time")
   if cl%2==0: _=axs[cl].set_ylabel("probability")
   _=axs[cl].set_title("ta"+str(startid)+"-"+str(endid-1),fontsize=10)   
   if cl==0: _=axs[cl].legend(bbox_to_anchor=(0., 1.12, 2.11, 1.12), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)


#plt.show()
#plt.close()
plt.savefig("figs/compare-"+algoA+"-"+algoB+".png")  
plt.close()  
#+END_SRC

* Example3: Display multiple algorithms on a single plot           :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle no :exports all :results output
:END:



#+BEGIN_SRC python
import sys 
sys.path.append('./code')
from libGenerateTablesImages  import *

# working with Taillard instances
# define 4 classes of problems ta11-ta20, ta21-30,ta31-40,ta41-50
problemclass={0: [11,21],1:[21,31],2:[31,41],3:[41,51]}

fig, axs = plt.subplots(2,2, figsize=(15, 6), facecolor='w', edgecolor='k')
fig.subplots_adjust(hspace = 0.3, wspace=0.11)
axs = axs.ravel()
plotsymbsworse = ['r--','b--','g--','k--','y--','r-','b-','g-','k-','y-','r+-','b+-','g+-','k+-','y+-']
plotsymbsbetter = ['r-','b-','g-']
plotsymbsdiff = ['r+-','b+-','g+-']

# names of the reference algorithm
# we plot difference of P(better)-P(worse) with respect to the reference algorithm
#algoref = '2016C01'
#algoref = 'tenure05'
algoref = 'tmin05max20opt'

# exp_ids of algos to comapare with the reference algorithm
#algos = ['tenure05','tenure03','tenure04','tenure06','tenure07', 'tmin05max20opt', 'tmin05max40opt', 'tmin05max60opt']
#algos = ['2016T01','2016T02','2016T03']
algos = ['tenure05','tmin05max60opt','tmin05max80opt']


for cl in range(4):
   startid = problemclass[cl][0]
   endid = problemclass[cl][1]
   print("reading problems ta"+str(startid)+" to "+str(endid))
   problemsetnames = ['ta'+str(i)+'.txt' for i in range(startid,endid)]
   # get all relevant data
   dataset = readDBResults(problemsetnames, algos+[algoref], benchmark_path, db_path)
   temp_time = []   
   temp_probbetter = []      
   temp_probworse = []      
   temp_probdiff = []
   # get stats on #better,#worse,#same,#total
   # save results in a list of dictionaries
   # each dictionary key is a tuple (problem, time) and return stats
   # for a specific problem and time threshold
   stats = {}
   # look at the first 30 minutes in the data (range(1,31))
   for minute in range(1,31):
      # timelimit in seconds
      timelimit = minute*60
      temp_time.append(minute)      
      for idx,algo in enumerate(algos):
          temp_probbetter.append([])
          temp_probworse.append([])
          temp_probdiff.append([])    
          pbetter = 0
          psame = 0
          pworse = 0 
          for problem in problemsetnames:
              stats[(problem,timelimit)]=compareABSingleProblem(dataset, algo, algoref, problem, timelimit)
              if stats[(problem,minute*60)]['total']>0:
                  pbetter = pbetter + 1.0*(stats[(problem,minute*60)]['nbetter'] )/stats[(problem,minute*60)]['total']
                  psame = psame + 1.0*(stats[(problem,minute*60)]['nsame'] )/stats[(problem,minute*60)]['total']        
                  pworse = pworse + 1.0*(stats[(problem,minute*60)]['nworse'] )/stats[(problem,minute*60)]['total']        
              else: print("either "+algoref+" or "+algo+" lack data for problem "+problem)          
          temp_probbetter[idx].append(pbetter/len(problemsetnames))    
          temp_probworse[idx].append(pworse/len(problemsetnames))    
          temp_probdiff[idx].append((pbetter-pworse)/len(problemsetnames))    
          #temp_prob2.append(psame/len(problemsetnames))    
          #temp_prob3.append(pworse/len(problemsetnames))          
   # now plot
   # probability better
   for idx,algo in enumerate(algos):
       #_=axs[cl].plot(temp_time, temp_probbetter[idx], plotsymbsbetter[idx])
       #_=axs[cl].plot(temp_time, temp_probworse[idx], plotsymbsworse[idx])
       _=axs[cl].plot(temp_time, temp_probdiff[idx], plotsymbsworse[idx],label=algo)
       _=axs[cl].axhline(0.0,color='b',linestyle='dashed', linewidth=0.2)
   _=axs[cl].yaxis.set_ticks([-1,-0.5,0,0.5,1.0])
   _=axs[cl].axis([0, 31, -1.1, 1.1])      
   # plot axis labels but not everywhere
   if cl>1: _=axs[cl].set_xlabel("time")
   if cl%2==0: _=axs[cl].set_ylabel("risk difference")
   _=axs[cl].set_title("ta"+str(startid)+"-"+str(endid-1),fontsize=10)   
   # only plot the legend for one subplot
   #if cl==1: _=axs[cl].legend(bbox_to_anchor=(1.05, 0), loc='upper left', borderaxespad=0.)         
   if cl==0: _=axs[cl].legend(bbox_to_anchor=(0., 1.12, 2.11, 1.12), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)
   

#plt.show()
#plt.close()
fname = "figs/comparediff-ref"+algoref
for algo in algos:
    
    fname = fname + "-" + algo

fname = fname + ".png"
print(fname)

plt.savefig(fname)  
plt.close()  
#+END_SRC




#+BEGIN_SRC python
#plt.savefig("figs/comparediff-T01-T0204.png")  
#plt.close()  
fname = "figs/comparediff-ref"+algoref
for algo in algos:
    
    fname = fname + "-" + algo

fname = fname + ".png"
print(fname)
#+END_SRC

* Example4: Display multiple plot for risk differences with confidence interval :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle no :exports all :results output
:END:

#+BEGIN_SRC python
import sys
sys.path.append('./code')
from libGenerateTablesImages  import *
import matplotlib as mpl
mpl.rcParams['text.usetex'] = True

# Example which is plotting differences probability with confidence interval.
# The confidence interval in this plot is important.
# If the interval includes zero, it means the difference is not significant.
# If it does not include zero, it means the difference is significant.
# working with Taillard instances
# define 4 classes of problems ta11-ta20, ta21-30,ta31-40,ta41-50
# This example plots each class in different plots
problemclass={0: [11,21],1:[21,31],2:[31,41],3:[41,51]}

# Plot settings
plot_row = int((len(problemclass)+1)/2)
fig, axs = plt.subplots(plot_row,2, figsize=(14, plot_row*3), facecolor='w', edgecolor='k')
fig.subplots_adjust(hspace = 0.3, wspace=0.11)
axs = axs.ravel()
plotsymbsworse = ['b--','r--','g--','k--','y--','r-','b-','g-','k-','y-','r+-','b+-','g+-','k+-','y+-']
plotsymbsbetter = ['b-','r-','g-']
plotsymbsdiff = ['b+-','r+-','g+-']

# Confidence Interval Method
conf_mtd = 4
# 1: Asymptotic with Continuity 
# 2: Asymptotic without Continuity
# 3: Wilson's Score without Continuity
# 4: Wilson's Score with Continuity
# 5: Corrected Wilson's Score with Continuity (in this case it is the same as 3: Wilson's Score without Continuity)
conf_int = 0.95

# names of the reference algorithm
# we plot difference of P(better)-P(worse) with respect to the reference algorithm
algoref = '2016T04'

# exp_ids of algos to comapare with the reference algorithm
algos = ['2016C04']

for cl in range(len(problemclass)):
    startid = problemclass[cl][0]
    endid = problemclass[cl][1]
    print("reading problems ta"+str(startid)+" to "+str(endid))
    problemsetnames = ['jsp_problems/ta'+str(i)+'.txt' for i in range(startid,endid)]
    # get all relevant data
    dataset = readDBResults(problemsetnames, algos+[algoref], benchmark_path, db_path)
    temp_time = []   
    temp_probbetter = []      
    temp_probworse = []      
    temp_probdiff = []
    temp_probmin = []
    temp_probmax = []
    # get stats on #better,#worse,#same,#total
    # save results in a list of dictionaries
    # each dictionary key is a tuple (problem, time) and return stats
    # for a specific problem and time threshold
    stats = {}
    # look at the first 30 minutes in the data (range(1,31))
    for minute in range(1,31):
        # timelimit in seconds
        timelimit = minute*60
        temp_time.append(minute)      
        for idx,algo in enumerate(algos):
            temp_probbetter.append([])
            temp_probworse.append([])
            temp_probdiff.append([])
            temp_probmin.append([])
            temp_probmax.append([])
            nbetter = 0
            nsame = 0
            nworse = 0
            ntotal = 0
            pbetter = 0
            psame = 0
            pworse = 0 
            for problem in problemsetnames:
                stats[(problem,timelimit)]=compareABSingleProblem(dataset, algo, algoref, problem, timelimit)
                if stats[(problem,minute*60)]['total']>0:
                    nbetter = nbetter + stats[(problem,minute*60)]['nbetter']
                    nsame = nsame + stats[(problem,minute*60)]['nsame']
                    nworse = nworse + stats[(problem,minute*60)]['nworse']
                    ntotal = ntotal + stats[(problem,minute*60)]['total']
                    pbetter = pbetter + 1.0*(stats[(problem,minute*60)]['nbetter'] )/stats[(problem,minute*60)]['total']
                    psame = psame + 1.0*(stats[(problem,minute*60)]['nsame'] )/stats[(problem,minute*60)]['total']        
                    pworse = pworse + 1.0*(stats[(problem,minute*60)]['nworse'] )/stats[(problem,minute*60)]['total']        
                else:
                    print("either "+algoref+" or "+algo+" lack data for problem "+problem)
            temp_probbetter[idx].append(pbetter/len(problemsetnames))    
            temp_probworse[idx].append(pworse/len(problemsetnames))    
            temp_probdiff[idx].append((pbetter-pworse)/len(problemsetnames))
            if conf_mtd == 1 :
                (temp_probmin_temp, temp_probmax_temp, temp_title) = AsymptoticC(nbetter, nworse, nsame, ntotal, conf_int)
            elif conf_mtd == 2 :
                (temp_probmin_temp, temp_probmax_temp, temp_title) = AsymptoticNC(nbetter, nworse, nsame, ntotal, conf_int)
            elif conf_mtd == 3 :
                (temp_probmin_temp, temp_probmax_temp, temp_title) = WilsonNC(nbetter, nworse, nsame, ntotal, conf_int)
            elif conf_mtd == 4 :
                (temp_probmin_temp, temp_probmax_temp, temp_title) = WilsonC(nbetter, nworse, nsame, ntotal, conf_int)
            elif conf_mtd == 5 :
                (temp_probmin_temp, temp_probmax_temp, temp_title) = WilsonCC(nbetter, nworse, nsame, ntotal, conf_int)
            temp_probmin[idx].append(temp_probmin_temp)
            temp_probmax[idx].append(temp_probmax_temp)          
    # now plot
    # probability better
    for idx,algo in enumerate(algos):
        temp_label = algo
        temp_label = temp_label.replace("2016C", "DTA")
        temp_label = r'$P_{\mathcal A < \mathcal B|\mathcal C} - P_{\mathcal B < \mathcal A|\mathcal C}$'
        _=axs[cl].plot(temp_time, temp_probdiff[idx], plotsymbsworse[idx],label=temp_label)
        _=axs[cl].fill_between(temp_time, temp_probmin[idx], temp_probmax[idx], color=plotsymbsworse[idx][0], alpha=0.2)
    _=axs[cl].axhline(0.0,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].axhline(-1.0,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].axhline(1.0,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].yaxis.set_ticks([-1,-0.5,0,0.5,1.0])
    _=axs[cl].axis([0, 31, -1.1, 1.1])      
    # plot axis labels but not everywhere
    if cl>=(2*(plot_row - 1)): _=axs[cl].set_xlabel("time (minute)")
    if cl%2==0: _=axs[cl].set_ylabel("risk difference")
    _=axs[cl].set_title("ta"+str(startid)+"-"+str(endid-1),fontsize=10)   
    # only plot the legend for one subplot      
    if cl==0: _=axs[cl].legend(bbox_to_anchor=(0., 1.12, 2.11, 1.12), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)

fname = "figs/comparediff-detail-ref"+algoref

for algo in algos:
    fname = fname + "-" + algo

fname = fname + "-" + temp_title
fname = fname + ".png"

print(fname)

plt.savefig(fname)  
plt.close()
#+END_SRC

#+RESULTS:
: 
: >>> >>> >>> >>> >>> ... ... ... ... ... ... ... >>> >>> ... >>> >>> >>> >>> >>> >>> >>> >>> ... >>> ... ... ... ... ... >>> >>> ... ... >>> >>> ... >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... reading problems ta11 to 21
: reading problems ta21 to 31
: reading problems ta31 to 41
: reading problems ta41 to 51
: ... ... >>> >>> >>> >>> figs/comparediff-detail-ref2016T04-2016C04-Wilson-w-Cont.png

* Example5: Display single plot for risk differences probability with confidence interval :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle no :exports all :results output
:END:


#+BEGIN_SRC python
import sys
sys.path.append('./code')
from libGenerateTablesImages  import *
import matplotlib as mpl
mpl.rcParams['text.usetex'] = True

# Example which is plotting differences probability performance with confidence interval.
# The confidence interval in this plot is important.
# If the interval includes zero, it means the difference is not significant.
# If it does not include zero, it means the difference is significant.
# working with Taillard instances
# define 4 classes of problems ta11-ta20, ta21-30,ta31-40,ta41-50
# This example plots a different plot on all problems
problemclass={0: [11,21],1:[21,31],2:[31,41],3:[41,51]}
problemsetnames = []

# Plot settings
plotsymbsworse = ['b--','r--','g--','k--','y--','r-','b-','g-','k-','y-','r+-','b+-','g+-','k+-','y+-']
plotsymbsbetter = ['b-','r-','g-']
plotsymbsdiff = ['b+-','r+-','g+-']

# Confidence Interval Method
conf_mtd = 4
# 1: Asymptotic with Continuity 
# 2: Asymptotic without Continuity
# 3: Wilson's Score without Continuity
# 4: Wilson's Score with Continuity
# 5: Corrected Wilson's Score with Continuity (in this case it is the same as 3: Wilson's Score without Continuity)
conf_int = 0.95

# names of the reference algorithm
# we plot difference of P(better)-P(worse) with respect to the reference algorithm
algoref = '2016T04'

# exp_ids of algos to comapare with the reference algorithm
algos = ['2016C04']


for cl in range(len(problemclass)):
    startid = problemclass[cl][0]
    endid = problemclass[cl][1]
    for i in range(startid,endid):
        problemsetnames.append('jsp_problems/ta'+str(i)+'.txt')

# get all relevant data
dataset = readDBResults(problemsetnames, algos+[algoref], benchmark_path, db_path)
temp_time = []   
temp_probbetter = []      
temp_probworse = []      
temp_probdiff = []
temp_probmin = []
temp_probmax = []
# get stats on #better,#worse,#same,#total
# save results in a list of dictionaries
# each dictionary key is a tuple (problem, time) and return stats
# for a specific problem and time threshold
stats = {}
# look at the first 30 minutes in the data (range(1,31))
for minute in range(1,31):
    if (minute%10 == 0):
        print("working on minutes ", minute)
    # timelimit in seconds
    timelimit = minute*60
    temp_time.append(minute)      
    for idx,algo in enumerate(algos):
        temp_probbetter.append([])
        temp_probworse.append([])
        temp_probdiff.append([])
        temp_probmin.append([])
        temp_probmax.append([])
        nbetter = 0
        nsame = 0
        nworse = 0
        ntotal = 0
        pbetter = 0
        psame = 0
        pworse = 0 
        for problem in problemsetnames:
            stats[(problem,timelimit)]=compareABSingleProblem(dataset, algo, algoref, problem, timelimit)
            if stats[(problem,minute*60)]['total']>0:
                nbetter = nbetter + stats[(problem,minute*60)]['nbetter']
                nsame = nsame + stats[(problem,minute*60)]['nsame']
                nworse = nworse + stats[(problem,minute*60)]['nworse']
                ntotal = ntotal + stats[(problem,minute*60)]['total']
                pbetter = pbetter + 1.0*(stats[(problem,minute*60)]['nbetter'] )/stats[(problem,minute*60)]['total']
                psame = psame + 1.0*(stats[(problem,minute*60)]['nsame'] )/stats[(problem,minute*60)]['total']        
                pworse = pworse + 1.0*(stats[(problem,minute*60)]['nworse'] )/stats[(problem,minute*60)]['total']        
            else: 
                print("either "+algoref+" or "+algo+" lack data for problem "+problem)
        temp_probbetter[idx].append(pbetter/len(problemsetnames))    
        temp_probworse[idx].append(pworse/len(problemsetnames))    
        temp_probdiff[idx].append((pbetter-pworse)/len(problemsetnames))
        if conf_mtd == 1 :
            (temp_probmin_temp, temp_probmax_temp, temp_title) = AsymptoticC(nbetter, nworse, nsame, ntotal, conf_int)
        elif conf_mtd == 2 :
            (temp_probmin_temp, temp_probmax_temp, temp_title) = AsymptoticNC(nbetter, nworse, nsame, ntotal, conf_int)
        elif conf_mtd == 3 :
            (temp_probmin_temp, temp_probmax_temp, temp_title) = WilsonNC(nbetter, nworse, nsame, ntotal, conf_int)
        elif conf_mtd == 4 :
            (temp_probmin_temp, temp_probmax_temp, temp_title) = WilsonC(nbetter, nworse, nsame, ntotal, conf_int)
        elif conf_mtd == 5 :
            (temp_probmin_temp, temp_probmax_temp, temp_title) = WilsonCC(nbetter, nworse, nsame, ntotal, conf_int)
        temp_probmin[idx].append(temp_probmin_temp)
        temp_probmax[idx].append(temp_probmax_temp)         

# now plot
# probability better
for idx,algo in enumerate(algos):
    temp_label = algo
    temp_label = temp_label.replace("2016C", "DTA")
    temp_label = r'$P_{\mathcal A < \mathcal B|\mathcal C} - P_{\mathcal B < \mathcal A|\mathcal C}$'
    _=plt.plot(temp_time, temp_probdiff[idx], plotsymbsworse[idx],label=temp_label)
    _=plt.fill_between(temp_time, temp_probmin[idx], temp_probmax[idx], color=plotsymbsworse[idx][0], alpha=0.2)

_=plt.axhline(0.0,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axhline(-1.0,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axhline(1.0,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axis([0, 31, -1.1, 1.1])
_=plt.xlabel("time (minute)")
_=plt.ylabel("risk difference")
_=plt.yticks([-1,-0.5,0,0.5,1.0])
_=plt.title(r'$\mathcal C$',fontsize=10)  
_=plt.legend(bbox_to_anchor=(0., 1.07, 1., .102), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)


fname = "figs/comparediff-all-ref"+algoref
for algo in algos:
    fname = fname + "-" + algo

fname = fname + "-" + temp_title
fname = fname + ".png"

print(fname)

plt.savefig(fname)  
plt.close()
#+END_SRC

#+RESULTS:
: 
: >>> >>> >>> >>> >>> ... ... ... ... ... ... ... >>> >>> >>> ... >>> >>> >>> >>> ... >>> ... ... ... ... ... >>> >>> ... ... >>> >>> ... >>> >>> >>> ... ... ... ... ... >>> ... >>> ... ... ... ... >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... working on minutes  10
: working on minutes  20
: working on minutes  30
: ... ... ... ... ... ... ... ... >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> ... ... >>> >>> >>> >>> figs/comparediff-all-ref2016T04-2016C04-Wilson-w-Cont.png

* Example6: Display multiple plot for performance probability with confidence interval :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle no :exports all :results output
:END:

#+BEGIN_SRC python
import sys
sys.path.append('./code')
from libGenerateTablesImages  import *
import matplotlib as mpl
mpl.rcParams['text.usetex'] = True

# Example which is plotting performance probability with confidence interval.
# The confidence interval in this plot is important.
# If the interval includes zero, it means the difference is not significant.
# If it does not include zero, it means the difference is significant.
# working with Taillard instances
# define 4 classes of problems ta11-ta20, ta21-30,ta31-40,ta41-50
# This example plots each class in different plots
problemclass={0: [11,21],1:[21,31],2:[31,41],3:[41,51]}

# Plot settings
plot_row = int((len(problemclass)+1)/2)
fig, axs = plt.subplots(plot_row,2, figsize=(14, plot_row*3), facecolor='w', edgecolor='k')
fig.subplots_adjust(hspace = 0.3, wspace=0.11)
axs = axs.ravel()
plotsymbsworse = ['b--','r--','g--','k--','y--','r-','b-','g-','k-','y-','r+-','b+-','g+-','k+-','y+-']
plotsymbsbetter = ['b-','r-','g-']
plotsymbsdiff = ['b+-','r+-','g+-']

# Confidence Interval Method
conf_mtd = 4
# 1: Wald Interval (Normal Interval)
# 2: Clopper–Pearson Interval (Exact Method)
# 3: Adjusted Wald Interval
# 4: Wilson's Score Interval
conf_int = 0.95

# names of the reference algorithm
# we plot difference of P(better)+P(same) with respect to the reference algorithm
algoref = '2016T04'

# exp_ids of algos to comapare with the reference algorithm
algos = ['2016C04']

for cl in range(len(problemclass)):
    startid = problemclass[cl][0]
    endid = problemclass[cl][1]
    print("reading problems ta"+str(startid)+" to "+str(endid))
    problemsetnames = ['jsp_problems/ta'+str(i)+'.txt' for i in range(startid,endid)]
    # get all relevant data
    dataset = readDBResults(problemsetnames, algos+[algoref], benchmark_path, db_path)
    temp_time = []   
    temp_probbetter = []      
    temp_probworse = []      
    temp_probnotw = []
    temp_probmin = []
    temp_probmax = []
    # get stats on #better,#worse,#same,#total
    # save results in a list of dictionaries
    # each dictionary key is a tuple (problem, time) and return stats
    # for a specific problem and time threshold
    stats = {}
    # look at the first 30 minutes in the data (range(1,31))
    for minute in range(1,31):
        # timelimit in seconds
        timelimit = minute*60
        temp_time.append(minute)
        for idx,algo in enumerate(algos):
            temp_probbetter.append([])
            temp_probworse.append([])
            temp_probnotw.append([])
            temp_probmin.append([])
            temp_probmax.append([])
            nbetter = 0
            nsame = 0
            nworse = 0
            ntotal = 0
            pbetter = 0
            psame = 0
            pworse = 0
            for problem in problemsetnames:
                stats[(problem,timelimit)]=compareABSingleProblem(dataset, algo, algoref, problem, timelimit)
                if stats[(problem,minute*60)]['total']>0:
                    nbetter = nbetter + stats[(problem,minute*60)]['nbetter']
                    nsame = nsame + stats[(problem,minute*60)]['nsame']
                    nworse = nworse + stats[(problem,minute*60)]['nworse']
                    ntotal = ntotal + stats[(problem,minute*60)]['total']
                    pbetter = pbetter + 1.0*(stats[(problem,minute*60)]['nbetter'] )/stats[(problem,minute*60)]['total']
                    psame = psame + 1.0*(stats[(problem,minute*60)]['nsame'] )/stats[(problem,minute*60)]['total']
                    pworse = pworse + 1.0*(stats[(problem,minute*60)]['nworse'] )/stats[(problem,minute*60)]['total']
                else:
                    print("either "+algoref+" or "+algo+" lack data for problem "+problem)
            temp_probbetter[idx].append(pbetter/len(problemsetnames))    
            temp_probworse[idx].append(pworse/len(problemsetnames))    
            temp_probnotw[idx].append((pbetter+psame)/len(problemsetnames))
            if conf_mtd == 1 :
                (temp_probmin_temp, temp_probmax_temp, temp_title) = WaldInterval(nbetter+nsame, ntotal, conf_int)
            elif conf_mtd == 2 :
                (temp_probmin_temp, temp_probmax_temp, temp_title) = CPInterval(nbetter+nsame, ntotal, conf_int)
            elif conf_mtd == 3 :
                (temp_probmin_temp, temp_probmax_temp, temp_title) = AdjustWald(nbetter+nsame, ntotal, conf_int)
            elif conf_mtd == 4 :
                (temp_probmin_temp, temp_probmax_temp, temp_title) = WilsonScore(nbetter+nsame, ntotal, conf_int)
            temp_probmin[idx].append(temp_probmin_temp)
            temp_probmax[idx].append(temp_probmax_temp)
    # now plot
    # probability better
    for idx,algo in enumerate(algos):
        temp_label = algo
        temp_label = temp_label.replace("2016C", "DTA")
        temp_label = r'$P_{\mathcal A\leq \mathcal B|\mathcal C}$'
        _=axs[cl].plot(temp_time, temp_probnotw[idx], plotsymbsworse[idx],label=temp_label)
        _=axs[cl].fill_between(temp_time, temp_probmin[idx], temp_probmax[idx], color=plotsymbsworse[idx][0], alpha=0.2)
    _=axs[cl].axhline(0.0,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].axhline(0.5,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].axhline(1.0,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].yaxis.set_ticks([0,0.5,1.0])
    _=axs[cl].axis([0, 31, -0.1, 1.1])
    # plot axis labels but not everywhere
    if cl>=(2*(plot_row - 1)): _=axs[cl].set_xlabel("time (minute)")
    if cl%2==0: _=axs[cl].set_ylabel("probability")
    _=axs[cl].set_title("ta"+str(startid)+"-"+str(endid-1),fontsize=10)   
    # only plot the legend for one subplot
    #if cl==1: _=axs[cl].legend(bbox_to_anchor=(1.05, 0), loc='upper left', borderaxespad=0.)         
    if cl==0: _=axs[cl].legend(bbox_to_anchor=(0., 1.12, 2.11, 1.12), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)

fname = "figs/comparenotworse-detail-ref"+algoref
for algo in algos:  
    fname = fname + "-" + algo

fname = fname + "-" + temp_title
fname = fname + ".png"
print(fname)

plt.savefig(fname)  
plt.close()
#+END_SRC

#+RESULTS:
: 
: >>> >>> >>> >>> >>> ... ... ... ... ... ... ... >>> >>> ... >>> >>> >>> >>> >>> >>> >>> >>> ... >>> ... ... ... ... >>> >>> ... ... >>> >>> ... >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... reading problems ta11 to 21
: reading problems ta21 to 31
: reading problems ta31 to 41
: reading problems ta41 to 51
: ... ... >>> >>> >>> figs/comparenotworse-detail-ref2016T04-2016C04-Wilsons-Score.png

* Example7: Display single plot for performance probability with confidence interval :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle no :exports all :results output
:END:

#+BEGIN_SRC python
import sys
sys.path.append('./code')
from libGenerateTablesImages  import *
import matplotlib as mpl
mpl.rcParams['text.usetex'] = True

# Example which is plotting performance probability with confidence interval.
# The confidence interval in this plot is important.
# If the interval includes zero, it means the difference is not significant.
# If it does not include zero, it means the difference is significant.
# working with Taillard instances
# define 4 classes of problems ta11-ta20, ta21-30,ta31-40,ta41-50
# This example plots a different plot on all problems
problemclass={0: [11,21],1:[21,31],2:[31,41],3:[41,51]}
# problemclass={0: [11,12]}
problemsetnames = []

# Plot settings
plotsymbsworse = ['b--','r--','g--','k--','y--','r-','b-','g-','k-','y-','r+-','b+-','g+-','k+-','y+-']
plotsymbsbetter = ['b-','r-','g-']
plotsymbsdiff = ['b+-','r+-','g+-']

# Confidence Interval Method
conf_mtd = 3
# 1: Wald Interval (Normal Interval)
# 2: Clopper–Pearson Interval (Exact Method)
# 3: Adjusted Wald Interval
# 4: Wilson's Score Interval
conf_int = 0.95

# names of the reference algorithm
# we plot difference of P(better)+P(same) with respect to the reference algorithm
algoref = '2016T04'

# exp_ids of algos to comapare with the reference algorithm
algos = ['2016C04']
# algos = ['2016T04']

for cl in range(len(problemclass)):
    startid = problemclass[cl][0]
    endid = problemclass[cl][1]
    for i in range(startid,endid):
        problemsetnames.append('jsp_problems/ta'+str(i)+'.txt')

# get all relevant data
dataset = readDBResults(problemsetnames, algos+[algoref], benchmark_path, db_path)
temp_time = []
temp_probbetter = []
temp_probworse = []      
temp_probnotw = []
temp_probmin = []
temp_probmax = []
# get stats on #better,#worse,#same,#total
# save results in a list of dictionaries
# each dictionary key is a tuple (problem, time) and return stats
# for a specific problem and time threshold
stats = {}
# look at the first 30 minutes in the data (range(1,31))
for minute in range(1,31):
    if (minute%10 == 0):
        print("working on minutes ", minute)
    # timelimit in seconds
    timelimit = minute*60
    temp_time.append(minute)      
    for idx,algo in enumerate(algos):
        temp_probbetter.append([])
        temp_probworse.append([])
        temp_probnotw.append([])
        temp_probmin.append([])
        temp_probmax.append([])
        nbetter = 0
        nsame = 0
        nworse = 0
        ntotal = 0
        pbetter = 0
        psame = 0
        pworse = 0 
        for problem in problemsetnames:
            stats[(problem,timelimit)]=compareABSingleProblem(dataset, algo, algoref, problem, timelimit)
            if stats[(problem,minute*60)]['total']>0:
                nbetter = nbetter + stats[(problem,minute*60)]['nbetter']
                nsame = nsame + stats[(problem,minute*60)]['nsame']
                nworse = nworse + stats[(problem,minute*60)]['nworse']
                ntotal = ntotal + stats[(problem,minute*60)]['total']
                pbetter = pbetter + 1.0*(stats[(problem,minute*60)]['nbetter'] )/stats[(problem,minute*60)]['total']
                psame = psame + 1.0*(stats[(problem,minute*60)]['nsame'] )/stats[(problem,minute*60)]['total']        
                pworse = pworse + 1.0*(stats[(problem,minute*60)]['nworse'] )/stats[(problem,minute*60)]['total']        
            else: 
                print("either "+algoref+" or "+algo+" lack data for problem "+problem)
        temp_probbetter[idx].append(pbetter/len(problemsetnames))    
        temp_probworse[idx].append(pworse/len(problemsetnames))    
        temp_probnotw[idx].append((pbetter+psame)/len(problemsetnames))
        if conf_mtd == 1 :
            (temp_probmin_temp, temp_probmax_temp, temp_title) = WaldInterval(nbetter+nsame, ntotal, conf_int)
        elif conf_mtd == 2 :
            (temp_probmin_temp, temp_probmax_temp, temp_title) = CPInterval(nbetter+nsame, ntotal, conf_int)
        elif conf_mtd == 3 :
            (temp_probmin_temp, temp_probmax_temp, temp_title) = AdjustWald(nbetter+nsame, ntotal, conf_int)
        elif conf_mtd == 4 :
            (temp_probmin_temp, temp_probmax_temp, temp_title) = WilsonScore(nbetter+nsame, ntotal, conf_int)           
        temp_probmin[idx].append(temp_probmin_temp)
        temp_probmax[idx].append(temp_probmax_temp)         

# now plot
# probability better
for idx,algo in enumerate(algos):
    temp_label = algo
    temp_label = temp_label.replace("2016C", "DTA")
    temp_label = r'$P_{\mathcal A\leq \mathcal B|\mathcal C}$'
    _=plt.plot(temp_time, temp_probnotw[idx], plotsymbsworse[idx],label=temp_label)
    _=plt.fill_between(temp_time, temp_probmin[idx], temp_probmax[idx], color=plotsymbsworse[idx][0], alpha=0.2)

_=plt.axhline(0.0,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axhline(0.5,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axhline(1.0,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axis([0, 31, -0.1, 1.1])      
_=plt.xlabel("time (minute)")
_=plt.ylabel("probability")
_=plt.yticks([0,0.5,1.0])
_=plt.title(r'$\mathcal C$',fontsize=10)  
_=plt.legend(bbox_to_anchor=(0., 1.07, 1., .102), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)

fname = "figs/comparenotworse-all-ref"+algoref
for algo in algos:
    fname = fname + "-" + algo

fname = fname + "-" + temp_title
fname = fname + ".png"
print(fname)

plt.savefig(fname)  
plt.close()
#+END_SRC

#+RESULTS:
: 
: >>> >>> >>> >>> >>> ... ... ... ... ... ... ... >>> ... >>> >>> ... >>> >>> >>> >>> ... >>> ... ... ... ... >>> >>> ... ... >>> >>> ... >>> ... >>> ... ... ... ... ... >>> ... >>> ... ... ... ... >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... working on minutes  10
: working on minutes  20
: working on minutes  30
: ... ... ... ... ... ... ... ... >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> ... ... >>> >>> >>> figs/comparenotworse-all-ref2016T04-2016C04-Adjusted-Wald-Interval.png

* Example8: Bootstrap Display single plot for performance probability with confidence interval :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle no :exports all :results output
:END:

#+BEGIN_SRC python
import sys
sys.path.append('./code')
from libGenerateTablesImages  import *
from scikits import bootstrap
import matplotlib as mpl
mpl.rcParams['text.usetex'] = True

# Example which is plotting performance probability with confidence interval.
# The confidence interval in this plot is important.
# If the interval includes zero, it means the difference is not significant.
# If it does not include zero, it means the difference is significant.
# working with Taillard instances
# define 4 classes of problems ta11-ta20, ta21-30,ta31-40,ta41-50
# This example plots a different plot on all problems
problemclass={0: [11,21],1:[21,31],2:[31,41],3:[41,51]}
# problemclass={0: [11,21]}
problemsetnames = []

# Plot settings
plotsymbsworse = ['b--','r--','g--','k--','y--','r-','b-','g-','k-','y-','r+-','b+-','g+-','k+-','y+-']
plotsymbsbetter = ['b-','r-','g-']
plotsymbsdiff = ['b+-','r+-','g+-']

# Confidence Interval Method
conf_mtd = 5
# 1: Wald Interval (Normal Interval)
# 2: Clopper–Pearson Interval (Exact Method)
# 3: Adjusted Wald Interval
# 4: Wilson's Score Interval
# 5: Bootstrap Interval
conf_int = 0.95

# Bootstrap Replications
# (B + 1)*alpha/2 should be integer
B = 9999
# names of the reference algorithm
# we plot difference of P(better)+P(same) with respect to the reference algorithm
algoref = '2016T04'

# exp_ids of algos to comapare with the reference algorithm
algos = ['2016C04']
# algos = ['2016T04']

for cl in range(len(problemclass)):
    startid = problemclass[cl][0]
    endid = problemclass[cl][1]
    for i in range(startid,endid):
        problemsetnames.append('jsp_problems/ta'+str(i)+'.txt')

# get all relevant data
dataset = readDBResults(problemsetnames, algos+[algoref], benchmark_path, db_path)
fin_time = []
fin_probbetter = []
fin_probworse = []      
fin_probnotw = []
fin_probmin = []
fin_probmax = []
# get stats on #better,#worse,#same,#total
# save results in a list of dictionaries
# each dictionary key is a tuple (problem, time) and return stats
# for a specific problem and time threshold
# stats = {}
# look at the first 30 minutes in the data (range(1,31))
for minute in range(1,31):
    if (minute%10 == 0):
        print("working on minutes ", minute)
    # timelimit in seconds
    timelimit = minute*60
    fin_time.append(minute)
    for idx,algo in enumerate(algos):
#        print("comparing algorithm ", algo)
        fin_probbetter.append([])
        fin_probworse.append([])
        fin_probnotw.append([])
        fin_probmin.append([])
        fin_probmax.append([])
        temp_probbetter = []
        temp_probworse = []
        temp_probnotw = []
        temp_probmin = []
        temp_probmax = []
        stats = {}
        for problem in problemsetnames:
           panda_objective = {}
           panda_objective = genABobj(dataset, algo, algoref, problem, timelimit)
           for bsi in range(B):
              stats[(problem,bsi)] = compareABarr(panda_objective['objA'], panda_objective['objB'])
        for bsi in range(B):
#           if (bsi%1000 == 0): print("Bootstrap: ", bsi)
           nbetter = 0
           nsame = 0
           nworse = 0
           ntotal = 0
           pbetter = 0
           psame = 0
           pworse = 0
           for problem in problemsetnames:
               if stats[(problem,bsi)]['total']>0:
                   nbetter = nbetter + stats[(problem,bsi)]['nbetter']
                   nsame = nsame + stats[(problem,bsi)]['nsame']
                   nworse = nworse + stats[(problem,bsi)]['nworse']
                   ntotal = ntotal + stats[(problem,bsi)]['total']
                   pbetter = pbetter + 1.0*(stats[(problem,bsi)]['nbetter'] )/stats[(problem,bsi)]['total']
                   psame = psame + 1.0*(stats[(problem,bsi)]['nsame'] )/stats[(problem,bsi)]['total']        
                   pworse = pworse + 1.0*(stats[(problem,bsi)]['nworse'] )/stats[(problem,bsi)]['total']        
               else: 
                   print("either "+algoref+" or "+algo+" lack data for problem "+problem)
           temp_probbetter.append(pbetter/len(problemsetnames))    
           temp_probworse.append(pworse/len(problemsetnames))    
           temp_probnotw.append((pbetter+psame)/len(problemsetnames))
        fin_probmin[idx].append(np.percentile(temp_probnotw, ((1-conf_int)/2)*100, interpolation='nearest'))
        fin_probmax[idx].append(np.percentile(temp_probnotw, (1-(1-conf_int)/2)*100, interpolation='nearest'))
        fin_probnotw[idx].append(np.average(temp_probnotw))
#        [low_c, upp_c] = bootstrap.ci(temp_probnotw, np.average, alpha=0.05, n_samples=B, method='bca', output='lowhigh')
#        fin_probmin[idx].append(low_c)
#        fin_probmax[idx].append(upp_c)

# now plot
temp_title = "Bootstrap"
# probability better
for idx,algo in enumerate(algos):
   temp_label = algo
   temp_label = temp_label.replace("2016C", "DTA")
   temp_label = r'$P_{\mathcal A\leq \mathcal B|\mathcal C}$'
   _=plt.plot(fin_time, fin_probnotw[idx], plotsymbsworse[idx],label=temp_label)
   _=plt.fill_between(fin_time, fin_probmin[idx], fin_probmax[idx], color=plotsymbsworse[idx][0], alpha=0.2)

_=plt.axhline(0.0,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axhline(0.5,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axhline(1.0,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axis([0, 31, -0.1, 1.1])      
_=plt.xlabel("time (minute)")
_=plt.ylabel("probability")
_=plt.yticks([0,0.5,1.0])
_=plt.title(r'$\mathcal C$',fontsize=10)
_=plt.legend(bbox_to_anchor=(0., 1.07, 1., .102), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)

fname = "figs/comparenotworse-all-ref"+algoref
for algo in algos:
    fname = fname + "-" + algo

fname = fname + "-" + temp_title
fname = fname + ".png"
print(fname)

plt.savefig(fname)  
plt.close()
#+END_SRC

#+RESULTS:
: 
: >>> >>> >>> >>> >>> >>> ... ... ... ... ... ... ... >>> ... >>> >>> ... >>> >>> >>> >>> ... >>> ... ... ... ... ... >>> >>> ... ... >>> ... ... >>> >>> ... >>> ... >>> ... ... ... ... ... >>> ... >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... working on minutes  10
: working on minutes  20
: working on minutes  30
: ... >>> ... ... ... ... ... ... ... >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> ... ... >>> >>> >>> figs/comparenotworse-all-ref2016T04-2016C04-Bootstrap.png

* Example9: Bootstrap Display multiple plot for performance probability with confidence interval :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle no :exports all :results output
:END:

#+BEGIN_SRC python
import sys
sys.path.append('./code')
from libGenerateTablesImages  import *
import matplotlib as mpl
mpl.rcParams['text.usetex'] = True

# Example which is plotting performance probability with confidence interval.
# The confidence interval in this plot is important.
# If the interval includes zero, it means the difference is not significant.
# If it does not include zero, it means the difference is significant.
# working with Taillard instances
# define 4 classes of problems ta11-ta20, ta21-30,ta31-40,ta41-50
# This example plots each class in different plots
problemclass={0: [11,21],1:[21,31],2:[31,41],3:[41,51]}

# Plot settings
plot_row = int((len(problemclass)+1)/2)
fig, axs = plt.subplots(plot_row,2, figsize=(14, plot_row*3), facecolor='w', edgecolor='k')
fig.subplots_adjust(hspace = 0.3, wspace=0.11)
axs = axs.ravel()
plotsymbsworse = ['b--','r--','g--','k--','y--','r-','b-','g-','k-','y-','r+-','b+-','g+-','k+-','y+-']
plotsymbsbetter = ['b-','r-','g-']
plotsymbsdiff = ['b+-','r+-','g+-']

# Confidence Interval Method
conf_mtd = 5
# 1: Wald Interval (Normal Interval)
# 2: Clopper–Pearson Interval (Exact Method)
# 3: Adjusted Wald Interval
# 4: Wilson's Score Interval
# 5: Bootstrap Percentile
conf_int = 0.95

B = 9999
temp_title = "Bootstrap"
# names of the reference algorithm
# we plot difference of P(better)+P(same) with respect to the reference algorithm
algoref = '2016T04'

# exp_ids of algos to comapare with the reference algorithm
algos = ['2016C04']

for cl in range(len(problemclass)):
    startid = problemclass[cl][0]
    endid = problemclass[cl][1]
    print("reading problems ta"+str(startid)+" to "+str(endid))
    problemsetnames = ['jsp_problems/ta'+str(i)+'.txt' for i in range(startid,endid)]
    # get all relevant data
    dataset = readDBResults(problemsetnames, algos+[algoref], benchmark_path, db_path)
    fin_time = []   
    fin_probbetter = []      
    fin_probworse = []      
    fin_probnotw = []
    fin_probmin = []
    fin_probmax = []
    # get stats on #better,#worse,#same,#total
    # save results in a list of dictionaries
    # each dictionary key is a tuple (problem, time) and return stats
    # for a specific problem and time threshold
    #stats = {}
    # look at the first 30 minutes in the data (range(1,31))
    for minute in range(1,31):
        # timelimit in seconds
        timelimit = minute*60
        fin_time.append(minute)
        for idx,algo in enumerate(algos):
            fin_probbetter.append([])
            fin_probworse.append([])
            fin_probnotw.append([])
            fin_probmin.append([])
            fin_probmax.append([])
            temp_probbetter = []
            temp_probworse = []
            temp_probnotw = []
            temp_probmin = []
            temp_probmax = []
            stats = {}
            for problem in problemsetnames:
                panda_objective = {}
                panda_objective = genABobj(dataset, algo, algoref, problem, timelimit)
                for bsi in range(B):
                    stats[(problem,bsi)] = compareABarr(panda_objective['objA'], panda_objective['objB'])
            for bsi in range(B):
                nbetter = 0
                nsame = 0
                nworse = 0
                ntotal = 0
                pbetter = 0
                psame = 0
                pworse = 0
                for problem in problemsetnames:
                    if stats[(problem,bsi)]['total']>0:
                        nbetter = nbetter + stats[(problem,bsi)]['nbetter']
                        nsame = nsame + stats[(problem,bsi)]['nsame']
                        nworse = nworse + stats[(problem,bsi)]['nworse']
                        ntotal = ntotal + stats[(problem,bsi)]['total']
                        pbetter = pbetter + 1.0*(stats[(problem,bsi)]['nbetter'] )/stats[(problem,bsi)]['total']
                        psame = psame + 1.0*(stats[(problem,bsi)]['nsame'] )/stats[(problem,bsi)]['total']
                        pworse = pworse + 1.0*(stats[(problem,bsi)]['nworse'] )/stats[(problem,bsi)]['total']
                    else:
                        print("either "+algoref+" or "+algo+" lack data for problem "+problem)
                temp_probbetter.append(pbetter/len(problemsetnames))    
                temp_probworse.append(pworse/len(problemsetnames))    
                temp_probnotw.append((pbetter+psame)/len(problemsetnames))
            fin_probnotw[idx].append(np.average(temp_probnotw))
            fin_probmin[idx].append(np.percentile(temp_probnotw, ((1-conf_int)/2)*100, interpolation='nearest'))
            fin_probmax[idx].append(np.percentile(temp_probnotw, (1-(1-conf_int)/2)*100, interpolation='nearest'))
    # now plot
    # probability better
    for idx,algo in enumerate(algos):
        temp_label = algo
        temp_label = temp_label.replace("2016C", "DTA")
        temp_label = r'$P_{\mathcal A\leq \mathcal B|\mathcal C}$'
        _=axs[cl].plot(fin_time, fin_probnotw[idx], plotsymbsworse[idx],label=temp_label)
        _=axs[cl].fill_between(fin_time, fin_probmin[idx], fin_probmax[idx], color=plotsymbsworse[idx][0], alpha=0.2)
    _=axs[cl].axhline(0.0,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].axhline(0.5,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].axhline(1.0,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].yaxis.set_ticks([0,0.5,1.0])
    _=axs[cl].axis([0, 31, -0.1, 1.1])
    # plot axis labels but not everywhere
    if cl>=(2*(plot_row - 1)): _=axs[cl].set_xlabel("time (minute)")
    if cl%2==0: _=axs[cl].set_ylabel("probability")
    _=axs[cl].set_title("ta"+str(startid)+"-"+str(endid-1),fontsize=10)   
    # only plot the legend for one subplot
    #if cl==1: _=axs[cl].legend(bbox_to_anchor=(1.05, 0), loc='upper left', borderaxespad=0.)         
    if cl==0: _=axs[cl].legend(bbox_to_anchor=(0., 1.12, 2.11, 1.12), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)

fname = "figs/comparenotworse-detail-ref"+algoref
for algo in algos:  
    fname = fname + "-" + algo

fname = fname + "-" + temp_title
fname = fname + ".png"
print(fname)

plt.savefig(fname)  
plt.close()
#+END_SRC

#+RESULTS:
: 
: >>> >>> >>> >>> >>> ... ... ... ... ... ... ... >>> >>> ... >>> >>> >>> >>> >>> >>> >>> >>> ... >>> ... ... ... ... ... >>> >>> >>> >>> ... ... >>> >>> ... >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... reading problems ta11 to 21
: reading problems ta21 to 31
: reading problems ta31 to 41
: reading problems ta41 to 51
: ... ... >>> >>> >>> figs/comparenotworse-detail-ref2016T04-2016C04-Bootstrap.png

* Example10: Bootstrap Display multiple plot for risk differences with confidence interval :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle no :exports all :results output
:END:

#+BEGIN_SRC python
import sys
sys.path.append('./code')
from libGenerateTablesImages  import *
import matplotlib as mpl
mpl.rcParams['text.usetex'] = True

# Example which is plotting differences probability with confidence interval.
# The confidence interval in this plot is important.
# If the interval includes zero, it means the difference is not significant.
# If it does not include zero, it means the difference is significant.
# working with Taillard instances
# define 4 classes of problems ta11-ta20, ta21-30,ta31-40,ta41-50
# This example plots each class in different plots
problemclass={0: [11,21],1:[21,31],2:[31,41],3:[41,51]}

# Plot settings
plot_row = int((len(problemclass)+1)/2)
fig, axs = plt.subplots(plot_row,2, figsize=(14, plot_row*3), facecolor='w', edgecolor='k')
fig.subplots_adjust(hspace = 0.3, wspace=0.11)
axs = axs.ravel()
plotsymbsworse = ['b--','r--','g--','k--','y--','r-','b-','g-','k-','y-','r+-','b+-','g+-','k+-','y+-']
plotsymbsbetter = ['b-','r-','g-']
plotsymbsdiff = ['b+-','r+-','g+-']

# Confidence Interval Method
conf_mtd = 6
# 1: Asymptotic with Continuity 
# 2: Asymptotic without Continuity
# 3: Wilson's Score without Continuity
# 4: Wilson's Score with Continuity
# 5: Corrected Wilson's Score with Continuity (in this case it is the same as 3: Wilson's Score without Continuity)
# 6: Bootstrap quantile
conf_int = 0.95
B = 9999
temp_title = 'Bootstrap'

# names of the reference algorithm
# we plot difference of P(better)-P(worse) with respect to the reference algorithm
algoref = '2016T04'

# exp_ids of algos to comapare with the reference algorithm
algos = ['2016C04']

for cl in range(len(problemclass)):
    startid = problemclass[cl][0]
    endid = problemclass[cl][1]
    print("reading problems ta"+str(startid)+" to "+str(endid))
    problemsetnames = ['jsp_problems/ta'+str(i)+'.txt' for i in range(startid,endid)]
    # get all relevant data
    dataset = readDBResults(problemsetnames, algos+[algoref], benchmark_path, db_path)
    fin_time = []   
    fin_probbetter = []      
    fin_probworse = []      
    fin_probdiff = []
    fin_probmin = []
    fin_probmax = []
    # get stats on #better,#worse,#same,#total
    # save results in a list of dictionaries
    # each dictionary key is a tuple (problem, time) and return stats
    # for a specific problem and time threshold
    # stats = {}
    # look at the first 30 minutes in the data (range(1,31))
    for minute in range(1,31):
        # timelimit in seconds
        timelimit = minute*60
        fin_time.append(minute)      
        for idx,algo in enumerate(algos):
            fin_probbetter.append([])
            fin_probworse.append([])
            fin_probdiff.append([])
            fin_probmin.append([])
            fin_probmax.append([])
            temp_probbetter = []
            temp_probworse = []
            temp_probnotw = []
            temp_probdiff = []
            temp_probmin = []
            temp_probmax = []
            stats = {}
            for problem in problemsetnames:
                panda_objective = {}
                panda_objective = genABobj(dataset, algo, algoref, problem, timelimit)
                for bsi in range(B):
                    stats[(problem,bsi)] = compareABarr(panda_objective['objA'], panda_objective['objB'])
            for bsi in range(B):
                nbetter = 0
                nsame = 0
                nworse = 0
                ntotal = 0
                pbetter = 0
                psame = 0
                pworse = 0 
                for problem in problemsetnames:
                    if stats[(problem,bsi)]['total']>0:
                        nbetter = nbetter + stats[(problem,bsi)]['nbetter']
                        nsame = nsame + stats[(problem,bsi)]['nsame']
                        nworse = nworse + stats[(problem,bsi)]['nworse']
                        ntotal = ntotal + stats[(problem,bsi)]['total']
                        pbetter = pbetter + 1.0*(stats[(problem,bsi)]['nbetter'] )/stats[(problem,bsi)]['total']
                        psame = psame + 1.0*(stats[(problem,bsi)]['nsame'] )/stats[(problem,bsi)]['total']        
                        pworse = pworse + 1.0*(stats[(problem,bsi)]['nworse'] )/stats[(problem,bsi)]['total']        
                    else:
                        print("either "+algoref+" or "+algo+" lack data for problem "+problem)
                temp_probbetter.append(pbetter/len(problemsetnames))
                temp_probworse.append(pworse/len(problemsetnames))
                temp_probdiff.append((pbetter-pworse)/len(problemsetnames))
            fin_probdiff[idx].append(np.average(temp_probdiff))
            fin_probmin[idx].append(np.percentile(temp_probdiff, ((1-conf_int)/2)*100, interpolation='nearest'))
            fin_probmax[idx].append(np.percentile(temp_probdiff, (1-(1-conf_int)/2)*100, interpolation='nearest'))
    # now plot
    # probability better
    for idx,algo in enumerate(algos):
        temp_label = algo
        temp_label = temp_label.replace("2016C", "DTA")
        temp_label = r'$P_{\mathcal A < \mathcal B|\mathcal C} - P_{\mathcal B < \mathcal A|\mathcal C}$'
        _=axs[cl].plot(fin_time, fin_probdiff[idx], plotsymbsworse[idx],label=temp_label)
        _=axs[cl].fill_between(fin_time, fin_probmin[idx], fin_probmax[idx], color=plotsymbsworse[idx][0], alpha=0.2)
    _=axs[cl].axhline(0.0,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].axhline(-1.0,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].axhline(1.0,color='k',linestyle='dashed', linewidth=0.2)
    _=axs[cl].yaxis.set_ticks([-1,-0.5,0,0.5,1.0])
    _=axs[cl].axis([0, 31, -1.1, 1.1])      
    # plot axis labels but not everywhere
    if cl>=(2*(plot_row - 1)): _=axs[cl].set_xlabel("time (minute)")
    if cl%2==0: _=axs[cl].set_ylabel("risk difference")
    _=axs[cl].set_title("ta"+str(startid)+"-"+str(endid-1),fontsize=10)   
    # only plot the legend for one subplot      
    if cl==0: _=axs[cl].legend(bbox_to_anchor=(0., 1.12, 2.11, 1.12), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)

fname = "figs/comparediff-detail-ref"+algoref

for algo in algos:
    fname = fname + "-" + algo

fname = fname + "-" + temp_title
fname = fname + ".png"

print(fname)

plt.savefig(fname)  
plt.close()
#+END_SRC

#+RESULTS:
: 
: >>> >>> >>> >>> >>> ... ... ... ... ... ... ... >>> >>> ... >>> >>> >>> >>> >>> >>> >>> >>> ... >>> ... ... ... ... ... ... >>> >>> >>> >>> ... ... >>> >>> ... >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... reading problems ta11 to 21
: reading problems ta21 to 31
: reading problems ta31 to 41
: reading problems ta41 to 51
: ... ... >>> >>> >>> >>> figs/comparediff-detail-ref2016T04-2016C04-Bootstrap.png

* Example11: Bootstrap Display single plot for risk differences probability with confidence interval :NOEXPORT:
:PROPERTIES:
:EXPORT_LaTeX_CLASS: article
:EXPORT_EXCLUDE_TAGS: NOEXPORT
:EXPORT_OPTIONS: H:5 toc:nil date:nil
:EXPORT_AUTHOR: 
:header-args: :session gta-compare :tangle no :exports all :results output
:END:


#+BEGIN_SRC python
import sys
sys.path.append('./code')
from libGenerateTablesImages  import *
import matplotlib as mpl
mpl.rcParams['text.usetex'] = True

# Example which is plotting differences probability performance with confidence interval.
# The confidence interval in this plot is important.
# If the interval includes zero, it means the difference is not significant.
# If it does not include zero, it means the difference is significant.
# working with Taillard instances
# define 4 classes of problems ta11-ta20, ta21-30,ta31-40,ta41-50
# This example plots a different plot on all problems
problemclass={0: [11,21],1:[21,31],2:[31,41],3:[41,51]}
problemsetnames = []

# Plot settings
plotsymbsworse = ['b--','r--','g--','k--','y--','r-','b-','g-','k-','y-','r+-','b+-','g+-','k+-','y+-']
plotsymbsbetter = ['b-','r-','g-']
plotsymbsdiff = ['b+-','r+-','g+-']

# Confidence Interval Method
conf_mtd = 6
# 1: Asymptotic with Continuity 
# 2: Asymptotic without Continuity
# 3: Wilson's Score without Continuity
# 4: Wilson's Score with Continuity
# 5: Corrected Wilson's Score with Continuity (in this case it is the same as 3: Wilson's Score without Continuity)
# 6: Bootstrap percentile
conf_int = 0.95
B = 2
temp_title = 'Bootstrap'

# names of the reference algorithm
# we plot difference of P(better)-P(worse) with respect to the reference algorithm
algoref = '2016T04'

# exp_ids of algos to comapare with the reference algorithm
algos = ['2016C04']


for cl in range(len(problemclass)):
    startid = problemclass[cl][0]
    endid = problemclass[cl][1]
    for i in range(startid,endid):
        problemsetnames.append('jsp_problems/ta'+str(i)+'.txt')

# get all relevant data
dataset = readDBResults(problemsetnames, algos+[algoref], benchmark_path, db_path)
fin_time = []
fin_probdiff = []
fin_probbetter = []
fin_probworse = []
fin_probnotw = []
fin_probmin = []
fin_probmax = []
# get stats on #better,#worse,#same,#total
# save results in a list of dictionaries
# each dictionary key is a tuple (problem, time) and return stats
# for a specific problem and time threshold
# stats = {}
# look at the first 30 minutes in the data (range(1,31))
for minute in range(1,31):
    if (minute%10 == 0):
        print("working on minutes ", minute)
    # timelimit in seconds
    timelimit = minute*60
    fin_time.append(minute)      
    for idx,algo in enumerate(algos):
        fin_probbetter.append([])
        fin_probworse.append([])
        fin_probdiff.append([])
        fin_probmin.append([])
        fin_probmax.append([])
        temp_probdiff = []
        temp_probbetter = []
        temp_probworse = []
        temp_probnotw = []
        temp_probmin = []
        temp_probmax = []
        for problem in problemsetnames:
            panda_objective = {}
            panda_objective = genABobj(dataset, algo, algoref, problem, timelimit)
            for bsi in range(B):
                stats[(problem,bsi)] = compareABarr(panda_objective['objA'], panda_objective['objB'])
        for bsi in range(B):
            nbetter = 0
            nsame = 0
            nworse = 0
            ntotal = 0
            pbetter = 0
            psame = 0
            pworse = 0 
            for problem in problemsetnames:
                if stats[(problem,bsi)]['total']>0:
                    nbetter = nbetter + stats[(problem,bsi)]['nbetter']
                    nsame = nsame + stats[(problem,bsi)]['nsame']
                    nworse = nworse + stats[(problem,bsi)]['nworse']
                    ntotal = ntotal + stats[(problem,bsi)]['total']
                    pbetter = pbetter + 1.0*(stats[(problem,bsi)]['nbetter'] )/stats[(problem,minute*60)]['total']
                    psame = psame + 1.0*(stats[(problem,bsi)]['nsame'] )/stats[(problem,bsi)]['total']        
                    pworse = pworse + 1.0*(stats[(problem,bsi)]['nworse'] )/stats[(problem,bsi)]['total']        
                else: 
                    print("either "+algoref+" or "+algo+" lack data for problem "+problem)
            temp_probbetter.append(pbetter/len(problemsetnames))
            temp_probworse.append(pworse/len(problemsetnames))    
            temp_probdiff.append((pbetter-pworse)/len(problemsetnames))
        fin_probdiff[idx].append(np.average(temp_probdiff))
        fin_probmin[idx].append(np.percentile(temp_probdiff, ((1-conf_int)/2)*100, interpolation='nearest'))
        fin_probmax[idx].append(np.percentile(temp_probdiff, (1-(1-conf_int)/2)*100, interpolation='nearest'))

# now plot
# probability better
for idx,algo in enumerate(algos):
    temp_label = algo
    temp_label = temp_label.replace("2016C", "DTA")
    temp_label = r'$P_{\mathcal A < \mathcal B|\mathcal C} - P_{\mathcal B < \mathcal A|\mathcal C}$'
    print(len(fin_time), len(fin_probdiff[idx])
    _=plt.plot(fin_time, fin_probdiff[idx], plotsymbsworse[idx],label=temp_label)
    _=plt.fill_between(fin_time, fin_probmin[idx], fin_probmax[idx], color=plotsymbsworse[idx][0], alpha=0.2)

_=plt.axhline(0.0,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axhline(-1.0,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axhline(1.0,color='k',linestyle='dashed', linewidth=0.2)
_=plt.axis([0, 31, -1.1, 1.1])
_=plt.xlabel("time (minute)")
_=plt.ylabel("risk difference")
_=plt.yticks([-1,-0.5,0,0.5,1.0])
_=plt.title(r'$\mathcal C$',fontsize=10)  
_=plt.legend(bbox_to_anchor=(0., 1.07, 1., .102), loc=3, ncol=15, mode="expand", borderaxespad=0.,fontsize=10)


fname = "figs/comparediff-all-ref"+algoref
for algo in algos:
    fname = fname + "-" + algo

fname = fname + "-" + temp_title
fname = fname + ".png"

print(fname)

plt.savefig(fname)  
plt.close()
#+END_SRC

#+RESULTS:


* Python SandBox                                                   :NOEXPORT:
** Python Example Fake

Suppose we have two problems, and on the first one $\mathcal A$
dominates $\mathcal B$, but on the second one $\mathcal B$ dominated
$\mathcal A$. Assuming a flat prior $beta(1,1)$ the resulting
probability is $1/2 beta(a1,b1)+ 1/2 beta(a2,b2)
#+BEGIN_SRC python 
import numpy as np
import scipy
import matplotlib.pyplot as plt

nsamples = 100
nproblems = 2

a1=38
b1=22

a2=30
b2=30

x = 1.0/nproblems*np.random.beta(1+a1,1+b1,100000)+1.0/nproblems*np.random.beta(1+a2,1+b2,100000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='green', alpha=0.75)
_ = plt.xlabel('P(A< B)')
_ = plt.ylabel('Probability')
_ = plt.title(r'$\mathrm{Histogram\ of\ P(A< B)}$')

#plt.axis([40, 160, 0, 0.03])
plt.grid(True)
plt.savefig("figs/beta-example.png")
plt.close()
#+END_SRC

- check percentiles
#+BEGIN_SRC python
U=np.percentile(x,97.5)
L=np.percentile(x,2.5)
M=np.mean(x)
np.round(list([L,M,U]),3)


0.5*scipy.stats.beta.ppf( [.025,.5, .975], 1+a1, 1+b1)+0.5*scipy.stats.beta.ppf( [.025,.5, .975], 1+a2, 1+b2)
#+END_SRC

#+ATTR_ORG: :width 600
[[file:./figs/beta-example.png]]

- What about the bootstrap confidence interval? 

1. Resample a list of problems
2. For each of the selected problems, get a 1 or 0 values by sampling from all pairs

#+BEGIN_SRC R
set.seed(13)
n = 2

p = c(1,1,1,1,1,0,0,0,0,0,0,1,0,1)
#########################


library(boot)
my.statistic = function(data,i)
{
  d = data[i]
  print(d)
  m = mean(d)
}
B = 10^4-1



#bootobj = boot(data=p, statistic = my.statistic, R = B)
#boot.ci(bootobj, type = "all")
#+END_SRC

* THE END                                                          :NOEXPORT:



#  LocalWords:  LaTeX NOEXPORT pdf toc args mathcal optimality ATTR
#  LocalWords:  longtable ldots cdots displaystyle geq frac leq cdot
#  LocalWords:  quantile
#  LocalWords:  setspace graphicx nooneline raggedright figtopcap src
#  LocalWords:  subfigure multirow multicol amsfonts hyperref amsmath
#  LocalWords:  amssymb english emacs setq alist defun orgmode bibtex
#  LocalWords:  backend xelatex nonstopmode plist html matchers yas
#  LocalWords:  startup redisplay Battiti latestChange jth listH nl
#  LocalWords:  eqnarray nonumber subsubsection neq DTA FigDTA nfail
#  LocalWords:  niters SearchProcedure Alg FigTabu ENDFOR ENDIF impr
#  LocalWords:  ENDWHILE emptyset lastChanged infty iter TabuSet xnew
#  LocalWords:  modInd NonTabuSet choosen bmod param alg memoryless
#  LocalWords:  hline textbf cline Taillard's Taillard Grabowski HPC
#  LocalWords:  taillards NewtonHPC runtime Runtime Resende Ribeiro
#  LocalWords:  resende textsf bibliographystyle apalike TODO hesam
#  LocalWords:  JOBSHOP ALGO MAKESPAN RND gta os sqlite numpy np plt
#  LocalWords:  matplotlib pyplot pdb scipy uname oshylo elif SMP UTC
#  LocalWords:  lepidoptera readDBResults problemsetnames expids sql
#  LocalWords:  str jobshop len df DataFrame params dataset algoA sbs
#  LocalWords:  compareABSingleProblem algoB timelimit dictioanary ci
#  LocalWords:  nbetter nsame nworse dataframe bestobjectivesA sys
#  LocalWords:  groupby bestobjectivesB downsample problemsetids jsp
#  LocalWords:  statsOnSingleProblem libGenerateTablesImages txt axs
#  LocalWords:  tuple u'jsp dtype figsize facecolor edgecolor hspace
#  LocalWords:  wspace idx probbetter probsame probworse xlabel png
#  LocalWords:  ylabel fontsize savefig problemclass startid endid
#  LocalWords:  pbetter psame pworse SandBox nsamples nproblems
#  LocalWords:  normed mathrm Resample bootobj
