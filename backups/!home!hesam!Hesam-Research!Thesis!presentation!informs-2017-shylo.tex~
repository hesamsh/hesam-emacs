% Created 2018-02-01 Thu 10:15
\documentclass[presentation]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage[top=0.5in, bottom=0.5in, left=0.5in, right=0.5in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage[english]{babel}
\usepackage{multimedia}
\usepackage[boxed]{algorithm}
\usepackage{algorithmic}
\usepackage{multicol}
\usepackage{graphicx}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\usepackage{color}
\usepackage{url}
%\usepackage{hyperref}
\definecolor{linkcolour}{rgb}{0,0.2,0.6}
%\hypersetup{colorlinks,breaklinks,urlcolor=linkcolour, linkcolor=linkcolour}
\usepackage{pdfpages}
\usepackage{listings}
\usepackage{enumerate}
\definecolor{grey}{rgb}{0.9,0.9,0.9}
\definecolor{blueish}{RGB}{27,173,222}
\lstset{
keywordstyle=\color{blue},
commentstyle=\color{red},
backgroundcolor=\color{grey},
stringstyle=\color{blueish},
basicstyle=\ttfamily\small,
columns=fullflexible,
basewidth={0.5em,0.4em}
}
\RequirePackage{fancyvrb}
\DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}
\newcommand{\tb}{\textcolor[rgb]{0,0,0.7} }
\newcommand{\tbn}{\textcolor{blue}}
\newcommand{\tr}{\textcolor{red}}
\newtheorem{proposition}{Proposition}
\newtheorem{axiom}{Axiom}
\mode<presentation>{\usetheme{progressbar}
\progressbaroptions{titlepage=picture}
%\usetheme[width=2cm]{Montpellier}%{default}%
%\setbeamercovered{transparent}
% or whatever (possibly just delete it)
%  \usefonttheme[hoptionsi]{serif}
}
\progressbaroptions{imagename=UT-logo-trans.png}
\usepackage{amsmath,amssymb}
\usepackage[english]{babel}
% or whatever
\usepackage[latin1]{inputenc}
% or whatever
\usepackage{times}
\usepackage[T1]{fontenc}
%\usepackage{xmpmulti}
\usepackage{multimedia}
\usepackage[boxed]{algorithm}
\usepackage{algorithmic}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\newcommand{\tcr}{\textcolor{red}}
\newcommand{\mathcolor}{\textcolor[rgb]{0.6,0.01,.01} }
\newcommand{\tb}      {\textcolor[rgb]{0,0,0.7} }
\newcommand{\tbn}{\textcolor{blue}}
\newcommand{\tr}{\textcolor{red}}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.
%\usepackage{luximono}
%\usepackage{media9}
%\usepackage{pdfbase}
%\renewcommand*\familydefault{\ttdefault} %% Only if the base font of the document is to be typewriter style
\usetheme{default}
\author{\emph{Hesam Shams} \newline The University of Tennessee \newline  Department of Industrial \& Systems Engineering \newline Advisor: Dr. Oleg Shylo}
\date{\today}
\title{Learning Models for Discrete Optimization \newline}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 24.5.1 (Org mode 8.2.10)}}
\begin{document}

\maketitle

\begin{frame}[label=sec-1]{Outline}
\begin{itemize}
\item Overview
\item Learning models for optimization
\item Optimization algorithms with a learning component
\begin{itemize}
\item Directional Tabu Algorithm (DTA)
\end{itemize}
\item Experimental analysis of algorithms
\begin{itemize}
\item Compare DTA with a standard tabu
\end{itemize}
\item Parallel implementation
\item Conclusions and work plan
\end{itemize}
\end{frame}

\begin{frame}[label=sec-2]{Overview}
\begin{itemize}
\item Exact solvers
\item Automatic algorithms configuration
\item Search algorithms
\end{itemize}
\end{frame}

\begin{frame}[label=sec-3]{Exact Solvers}
\begin{itemize}
\item Mixed Integer Programming - MIP
\item Branch \& Bound (Strong branching - Node selection)
\item Problem structures
\end{itemize}
\begin{center}
\includegraphics[height=1.5in]{./BB.jpg}
\end{center}
\end{frame}
\begin{frame}[label=sec-4]{Automatic Algorithms Configuration}
\begin{block}{Parameter tuning}
\begin{itemize}
\item Manual vs Automatic Tuning
\end{itemize}
\end{block}
\begin{block}{Algorithm selection}
\begin{itemize}
\item Collection of available algorithms
\item Features of optimization problem
\end{itemize}
\end{block}
\end{frame}
\begin{frame}[label=sec-5]{Search Algorithms}
\begin{block}{Branch and Bound: Nore Rankings}
\begin{itemize}
\item Rating branching nodes
\item Tabu search
\end{itemize}
\end{block}
\begin{block}{Greedy search}
\begin{itemize}
\item Transform optimization problems to classification tasks
\item Update partial solutions
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label=sec-6]{Binary Optimization}
\begin{itemize}
\item Binary optimization problem
\end{itemize}
\begin{equation*}\label{general.binary.model}
\begin{array}{cc}
\text{minimize } f(x) \\
\text{s.t. } x \in S \subset \{0,1\}^n
\end{array}
\end{equation*}
\begin{itemize}
\item A feasible solution
\end{itemize}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0\\
\hline
\end{tabular}
\end{center}
\begin{itemize}
\item Classes of optimal
\end{itemize}
\begin{align*}
\mathcal C^1 &=\{j | x_j^*=1\} \\
\mathcal C^0 &=\{j | x_j^*=0\}
\end{align*}
\end{frame}
\begin{frame}[label=sec-7]{Information Structure}
\begin{itemize}
\item Discovered information
\end{itemize}
\begin{table}[]
\centering
\label{my-label}
\begin{tabular}{l|l|l|l|l|l|l}
component $j$ & 1 & 2 & 3 & 4 & 5 & objective function \\ \hline
Time 1    & 1 & 1 & 0 & 1 & 0 & 1388               \\ \hline
Time 2    & 0 & 1 & 0 & 0 & 1 & 1376               \\ \hline
Time 3    & 1 & 1 & 1 & 0 & 0 & 1381               \\ \hline
Time 4    & 0 & 1 & 0 & 0 & 0 & 1363               \\ \cline{2-6}
\end{tabular}
\end{table}

\begin{equation*}
I_j(t)=[ (x^1_j, f(x^1)), (x^2_j,f(x^2)), \ldots, (x^m_j, f(x^m))] \label{infromation.vec}
\end{equation*}

\begin{equation*}
I_1(t)=[ (1, 1388), (0, 1376), (1, 1381), (0, 1363)]
\end{equation*}
\end{frame}
\begin{frame}[label=sec-8]{Classification}
\begin{itemize}
\item Two classes of variables
\end{itemize}
\begin{align*}
\mathcal C^1 &=\{j | x_j^*=1\} \\
\mathcal C^0 &=\{j | x_j^*=0\}
\end{align*}
\begin{itemize}
\item Discovered information $I_j(t)$
\item Conditional probability
\end{itemize}
\begin{equation*}
Pr(j\in \mathcal C^1 | I_j(t))
\end{equation*}
\end{frame}
\begin{frame}[label=sec-9]{Logistic Regression}
\begin{itemize}
\item Conditional probability
\end{itemize}
\begin{equation*}
p(x)\equiv P(Y=1|x)
\end{equation*}
\begin{itemize}
\item Formulation
\end{itemize}
\begin{equation*}
\log{\frac{p(x)}{1-p(x)}} = \theta_0 + \theta x \label{logit:form}
\end{equation*}

\begin{equation*}
p(x;\theta_0,\theta) = \frac{1}{1+e^{-(\theta_0 + \theta x)}} \label{logit:fin}
\end{equation*}
\end{frame}

\begin{frame}[label=sec-10]{Logistic regression for optimization}
\begin{itemize}
\item Discovered information
\end{itemize}
\begin{align*}
I_j^1(t) &=\{ (x^k_j, f(x^k)): (x^k_j, f(x^k))\in I_j(t), x^k_j=1\} \\
I_j^0(t) &=\{ (x^k_j, f(x^k)): (x^k_j, f(x^k))\in I_j(t), x^k_j=0\}
\end{align*}

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
(1, 1350) & (1, 1363) & (1, 1366) & (1, 1341) & (1, 1338)\\
\hline
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
(0, 1349) & (0, 1366) & (0, 1341) & (0, 1337) & (0, 100000)\\
\hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}[label=sec-11]{Logistic regression for optimization}
\begin{itemize}
\item Sigmoid function
\end{itemize}
\begin{equation*}
h_{\theta}\left[ ( I_j(t) \right] =\frac{1}{1+e^G} \label{H.function}
\end{equation*}

\begin{equation*}
G=\displaystyle \sum_{ (1,f(x_k))\in I_j^1(t) } \theta f(x_k) - \displaystyle \sum_{ (0,f(x_k))\in I_j^0(t) } \theta f(x_k)
 \label{G.function}
\end{equation*}
\begin{block}{Computational issues}
\begin{itemize}
\item Memory usage
\item Optimal $\theta$ changes for different sizes of $I_j(t)$
\end{itemize}
\end{block}
\end{frame}
\begin{frame}[label=sec-12]{Reduced Linear Regression}
\begin{itemize}
\item Mapping information
\end{itemize}
\begin{align*}
D^1_j(t) &=M( \{ f(x_j^k) | (x_j^k, f(x^k)) \in I_j(t), x_j^k=1\}) \\
D^0_j(t) &=M( \{ f(x_j^k) | (x_j^k, f(x^k)) \in I_j(t), x_j^k=0\})
\end{align*}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
(1, 1350) & (1, 1363) & (1, 1366) & (1, 1341) & (1, 1338)\\
\hline
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
(0, 1349) & (0, 1366) & (0, 1341) & (0, 1337) & (0, 100000)\\
\hline
\end{tabular}
\end{center}
\begin{equation*}
\big\downarrow
\end{equation*}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
1338 & 1337\\
\hline
\end{tabular}
\end{center}
\begin{itemize}
\item Reduced version
\end{itemize}
\begin{equation*}
G= \theta D^1_j(t) - \theta D^0_j(t) \label{GA.function}
\end{equation*}
\end{frame}
\begin{frame}[label=sec-13]{Experiments}
\begin{itemize}
\item Job shop scheduling / Standard tabu search
\item Example of the training data:
\end{itemize}
\begin{center}
\begin{tabular}{rrr}
\hline
D$^{\text{1}}$(t) & D$^{\text{0}}$(t) & opt\\
\hline
1366 & 100000 & 1\\
1395 & 1366 & 0\\
1368 & 1400 & 1\\
1369 & 1380 & 1\\
1364 & 100000 & 1\\
1366 & 1438 & 1\\
1373 & 1366 & 0\\
1420 & 1366 & 0\\
1379 & 1365 & 0\\
1365 & 1389 & 1\\
\hline
\end{tabular}
\end{center}
\end{frame}
\begin{frame}[label=sec-14]{Computational results}
\begin{itemize}
\item Dataset was randomly split into the training and testing sets (152000 and 114000 records respectively)
\item Percentage of zero labels in the training set
\end{itemize}
\begin{center}
\begin{tabular}{rrr}
training set & test set & validation set\\
\hline
0.46 & 0.46 & 0.45\\
\end{tabular}
\end{center}
\begin{itemize}
\item The confusion matrix for the testing set predictions
\end{itemize}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
 & pred=0 & pred=1\\
\hline
real=0 & 0.95 & 0.05\\
real=1 & 0.08 & 0.92\\
\hline
\end{tabular}
\end{center}
\end{frame}
\begin{frame}[label=sec-15]{Heatmap of prediction}
\includegraphics[height=2.5in]{./plt-ta11-logisticcpredictions-based-on-f1avgf0avg-only-30min.png}
\end{frame}
\begin{frame}[label=sec-16]{Iterative Local Search}
\begin{block}{States}
\begin{itemize}
\item $s$: Solutions or configurations
\end{itemize}
\end{block}
\begin{block}{Neighborhood}
\begin{itemize}
\item $N(s)$: Neighbors of $S$
\end{itemize}
\end{block}
\begin{block}{Legal neighbors}
\begin{itemize}
\item $L(N(s),s)$: Set of legal functions
\end{itemize}
\end{block}
\begin{block}{Selection}
\begin{itemize}
\item $S(L(N(s),s),s)$: Selection function
\end{itemize}
\end{block}
\begin{block}{Objective function}
\begin{itemize}
\item Minimizing $f(s)$
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label=sec-17]{Graphical Illustration}
\end{frame}
\begin{frame}[label=sec-18]{Local Minima}
\begin{itemize}
\item $c$ is local minima
\end{itemize}
\begin{equation*}
\forall n \in N(c): f(n) \geq f(c) 
\end{equation*}
\begin{itemize}
\item No guarantees for global optimal
\end{itemize}
\end{frame}

\begin{frame}[label=sec-19]{Tabu Search Algorithm}
\begin{itemize}
\item Local search
\item Record visited nodes
\item Tabu list vs. Action list
\item Tabu tenure
\end{itemize}
\end{frame}

\begin{frame}[label=sec-20]{Tabu Improvement}
\begin{itemize}
\item Escaping from local minima
\item Guiding search to a good solution space
\item Dynamic tenure
\item Elite sets
\end{itemize}
\end{frame}
\begin{frame}[label=sec-21]{Directional Tabu Algorithm}
\begin{itemize}
\item Each component has a tenure
\item Dynamic tenure by logistic regression
\item Tenure sizes from $T^L$ to $T^U$
\end{itemize}
\end{frame}
\begin{frame}[label=sec-22]{Learning in Tabu}
\begin{itemize}
\item Reduced information
\end{itemize}
\begin{equation*}
[D^1_j(t), D^0_j(t)]
\end{equation*}
\begin{itemize}
\item Probability of each component
\end{itemize}
\begin{equation*}
\tilde{p}_j(\theta)  \equiv P\{x*_j=1\}=\frac{ 1}{1+e^{\theta D^1_j(t) - \theta D^0_j(t) }} 
\end{equation*}
\begin{itemize}
\item Example ($T^L=5$ and $T^U=9$)
\end{itemize}
\begin{table}[]
\centering
\begin{tabular}{l|l|l|l|l|}
\hline
Component   & 1     & 2     & 3     & 4     \\ \hline
Probability & 0.478 & 0.734 & 0.901 & 0.303 \\ \hline
Tenure      & 5     & 8     & 9     & 7     \\ \hline
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[label=sec-23]{Optimal Parameter $\theta$}
\begin{itemize}
\item The optimal solution of problems required
\item Interval $[\theta_{\min}, \theta_{\max}]$ estimates the location of optimal $\theta$
\item Algorithms scans a grid of points $\theta_{\min} \leq \theta_1 < \ldots < \theta_m \leq \theta_{\max}$
\item Temperature in simulated annealing method
\item Maximum likelihood estimator
\end{itemize}
\end{frame}
\begin{frame}[label=sec-24]{Optimal Parameter $\theta$}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\theta$$_{\min}$ & $\theta$$_{\text{1}}$ & \ldots & $\theta_m$ & $\theta$$_{\max}$\\
\hline
\end{tabular}
\end{center}
\begin{align*}
&\uparrow \\
&\theta^{opt}
\end{align*}

\begin{itemize}
\item The algorithm runs with $\theta$$_{\min}$
\item The algorithm runs with $\theta$$_{\text{1}}$
\item Until the algorithm runs with $\theta$$_{\text{max}}$
\item Maximum likelihood to estimate $\theta$$^{\text{opt}}$
\end{itemize}
\end{frame}
\begin{frame}[label=sec-25]{Job Shop Scheduling}
\begin{itemize}
\item $n$ jobs to be processed on $m$ machines
\item Each machine process on job at a time
\item NP-hardness: general TSP
\item Tiallard's benchmark
\end{itemize}
\begin{center}
\includegraphics[height=1in]{./jsexample.png}
\end{center}
\end{frame}
\begin{frame}[label=sec-26]{Analysis of Algorithms}
\begin{block}{Worst case analysis}
\begin{itemize}
\item Pessimistic prediction
\end{itemize}
\end{block}
\begin{block}{Average case analysis}
\begin{itemize}
\item Well-justified input
\end{itemize}
\end{block}
\begin{block}{Experimental analysis}
\begin{itemize}
\item Highlight strengths and weaknesses
\item Comparing with other methods
\item Benchmark problems
\item Tables with no clear comparison
\item Time threshold
\end{itemize}
\end{block}
\end{frame}
\begin{frame}[label=sec-27]{Main Idea}
Given 
\begin{itemize}
\item two algorithms $\mathcal A$ and $\mathcal B$
\item set of problems $\mathcal C$
\item time budget $T$
\end{itemize}

What is the probability that $\mathcal A$ produces a solution better than $\mathcal B$ if terminated after time $T$?
\end{frame}


\begin{frame}[label=sec-28]{Comparison of Algorithms Framework}
\begin{itemize}
\item Two algorithms $\mathcal A$ and $\mathcal B$ on set of problems $\mathcal C$
\item $\mathcal N$ runs of algorithm $\mathcal A$ and $\mathcal M$ runs of algorithm $\mathcal B$ for each problem instance
\item Record solutions with times
\item Pairwise comparison at each time unit
\item Enumerate better, worse, and same solutions
\item Two measures: performance probability and risk difference
\end{itemize}
\end{frame}

\begin{frame}[label=sec-29]{Comparisons}
\begin{itemize}
\item On a problem $c_i$ at time $t$
\item Random sample with replacement
\end{itemize}
\begin{align*}
X^{\mathcal A}_{c_i} &= (x^i_1, \ldots, x^i_n) \\
X^{\mathcal B}_{c_i} &= (y^i_1, \ldots, y^i_m)
\end{align*}
\begin{itemize}
\item $N_{\mathcal A < \mathcal B}|c_i$: Number of solutions $\mathcal A$ is better than $\mathcal B$
\item $N_{\mathcal A > \mathcal B}|c_i$: Number of solutions $\mathcal A$ is worse than $\mathcal B$
\item $N_{\mathcal A = \mathcal B}|c_i$: Number of solutions $\mathcal A$ is equal to $\mathcal B$
\item Aggregations on all problems
\end{itemize}
\end{frame}
\begin{frame}[label=sec-30]{Measures}
\begin{block}{Performance Probability}
\begin{itemize}
\item Probability of $\mathcal A$ has a larger or equal performance than $\mathcal B$
\item Between 0 to 1
\end{itemize}
\begin{align}
P_{\mathcal A\leq \mathcal B|\mathcal C} = \frac{N_{\mathcal A < \mathcal B|C} + N_{\mathcal A = \mathcal B|C}}{\mathcal N \times \mathcal M}
\end{align}
\end{block}
\begin{block}{Risk Difference}
\begin{itemize}
\item Difference probability of outperforming
\item Between -1 to +1
\end{itemize}
\begin{align}
RD_{\mathcal A < \mathcal B|\mathcal C} =\frac{N_{A<B|\mathcal C} - N_{A>B|\mathcal C}}{\mathcal N \times \mathcal M} 
\end{align}
\end{block}
\end{frame}
\begin{frame}[label=sec-31]{Bootstrap Confidence Interval}
\begin{itemize}
\item $f(\mathcal A,t) \sim Normal(\mu=100,\sigma=10)$
\item $f(\mathcal B,t) \sim Normal(\mu=100-t,\sigma=10)$
\end{itemize}
\includegraphics[height=2.0in]{./bootstrap-normal.png}
\end{frame}


\begin{frame}[label=sec-32]{Results - Performance Probability}
\begin{center}
\includegraphics[height=2.8in]{./comparenotworse-all-ref2016T04-2016C04-Bootstrap.png}
\end{center}
\end{frame}
\begin{frame}[label=sec-33]{Results - Risk Difference}
\begin{center}
\includegraphics[height=2.8in]{./comparediff-all-ref2016T04-2016C04-Bootstrap.png}
\end{center}
\end{frame}

\begin{frame}[label=sec-34]{Cooperative Solvers}
\includegraphics[height=1in]{nocomm.png}
\includegraphics[height=1in]{comm-structure1.png}
\includegraphics[height=1in]{comm-structure2.png}

\begin{itemize}
\item how can we capture communication dynamics?
\item what is the optimal communication structure?
\item what is the optimal frequency of communication?
\end{itemize}
\end{frame}

\begin{frame}[label=sec-35]{Modeling Communication}
\begin{itemize}
\item a collection of semi-Markov processes
\item semi-Markov processes can ``talk" to each other
\item semi-Markov processes can trigger some state transitions in other chains
\end{itemize}

\includegraphics[height=1.5in]{./smp-example2-alpha.png} 
\end{frame}

\begin{frame}[label=sec-36]{Linear Random Walk Model}
\includegraphics[width=.9\linewidth]{./linegraph-alpha.png}
\begin{itemize}
\item a finite set of ordered solutions, $x_1, x_2, \ldots x_m$
\item $x_1$ is the initial solution
\item $x_m$ is the terminal solution
\item after finding $x_i$, the time required to find $x_{i+1}$ is
\item $T_i$ -- trainsition time form $x_i$ to $x_{i+1}$
\end{itemize}
\end{frame}

\begin{frame}[label=sec-37]{Best Communication Strategy?}
\includegraphics[width=.9\linewidth]{./linegraph-alpha.png}
\includegraphics[width=.9\linewidth]{./linegraph-alpha.png}
\includegraphics[width=.9\linewidth]{./linegraph-alpha.png}
\end{frame}


\begin{frame}[label=sec-38]{Speedup Example}
\begin{itemize}
\item $p_i=10^{-4}$,  $R_i=1$ for $i\in \{1,2,\ldots,100\}$
\end{itemize}

\begin{table}[htb]
\caption{Speedup ratios with respect to the number of concurrent algorithms (size). \label{speedup.ratios}}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
Size & no communication & communication\\
\hline
8 & 1.2 & 8\\
64 & 1.3 & 63.8\\
128 & 1.3 & 127.2\\
512 & 1.4 & 499.1\\
1024 & 1.4 & 973.4\\
4096 & 1.5 & 3361\\
16384 & 1.5 & 8057.3\\
24576 & 1.6 & 9143.7\\
32768 & 1.6 & 9622.6\\
\hline
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[label=sec-39]{Modeling Optimization Algorithm}
\includegraphics[height=2.5in]{./smp-example-real-alpha.png}
\end{frame}


\begin{frame}[label=sec-40]{Conclusions}
\begin{itemize}
\item Logistic regression for binary optimization solvers
\item Statistical framework for comparing optimization algorithms
\item Directional Tabu Algorithm 
\begin{itemize}
\item learning vs no learning
\end{itemize}
\item Markov Models for Analyzing Algorithms
\end{itemize}
\end{frame}

\begin{frame}[label=sec-41]{Thanks}
\begin{block}{Questions}
\end{block}
\end{frame}
% Emacs 24.5.1 (Org mode 8.2.10)
\end{document}
